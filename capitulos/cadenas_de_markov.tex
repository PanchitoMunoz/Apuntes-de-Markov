%!TEX root = ../apuntesMarkov.tex

\section{Cadenas de Markov a tiempo discreto}

\subsection{Definición, construcción y propiedades básicas}

\subsubsection{Breve recuerdo de probabilidades}

Un \emph{espacio de probabilidad}\MarginNote{Espacio de Probabilidad} es un espacio de medida $(\Omega,\cf,\pp)$ tal que $\pp(\Omega)=1$.
$\pp$ es la \emph{medida de probabilidad}\MarginNote{Medida de Probabilidad}.
Los $A\in\cf$ son los \emph{eventos}.
\lsep
Dado un espacio medible $(S,\cs)$, una \emph{variable aleatoria}\MarginNote{Variable Aleatoria} $X$ a valores en $S$ es una función $X\!:\Omega\longrightarrow S$ que es $\cf$-$\cs$ medible.
Usualmente nos olvidamos del espacio de probabilidad en la notación.
Por ejemplo, cuando escribimos $\pp(X\in A)$ a lo que nos referimos es a la medida de la preimagen de $A$ por $X$, es decir
\[\pp(X\in A)=\pp(X^{-1}(A))=\pp(\{\omega\in \Omega\!:X(\omega)\in A\}).\]
Rara vez incluimos los $\omega$'s en la notación, pero es importante mantener la definición formal en mente.
\lsep Dos eventos $A$ y $B$ son \emph{independientes}\MarginNote{Independencia} si $\pp(A\cap B)=\pp(A)\pp(B)$.
La \emph{probabilidad condicional}\MarginNote{Prob. Condicional} de $A$ dado $B$ es
\[\pp(A|B)=\frac{\pp(A\cap B)}{\pp(B)}\]
(asumiendo $\pp(B)>0$; a menudo omitiremos especificar esto cuando hablamos de probabilidades condicionales).
\lsep
Notar que la aplicación $\pp(\cdot|B)\!:\cf\longrightarrow[0,1]$ define una medida de probabilidad\MarginNote{Med. de Prob. sobre un evento B} sobre $(\Omega,\cf,\pp)$ soportada en $B$ (o alternativamente una medida de probabilidad sobre $(B,\cf,\pp)$).
De esta forma, por ejemplo, podemos decir que dados $A,B,C\in\cf$, $A$ y $B$ son \emph{independientes condicionalmente a $C$} si $\pp(A\cap B|C)=\pp(A|C)\pp(B|C)$.
\uexer: Asegúrese de que comprende el contenido de la última frase.

% \begin{mdframed}[style=st_red]
\begin{sqgnote}
Repase la noción de independencia para variables aleatorias y familias de variables aleatorias.
\end{sqgnote}
% \mbox{}\vskip-30pt\mbox{}
\begin{sqgnote}
Repase la definición y propiedades de esperanza condicional, dadas en el Apéndice \ref{sec:espCond}.
\end{sqgnote}
% \end{mdframed}


\subsubsection{Procesos estocásticos y cadenas de Markov}

Dados $(\Omega,\cf,\pp)$ y $(S,\cs)$ como antes, decimos que una sucesión $(X_n)_{n\geq0}$ de variables aleatorias es un \emph{proceso estocástico}\MarginNote{Proceso Estocástico}.
Alternativamente uno puede pensar en el proceso estocástico $(X_n)_{n\geq0}$ como una variable aleatoria a valores en el espacio producto $S^\nn$, es decir $X\!:\Omega\longrightarrow S^\nn$ que es $\cf$-$\cs^\nn$ medible, con $\cs^\nn$ dotado de la $\sigma$-álgebra producto.
\lsep
(Más adelante consideraremos procesos estocásticos indexados por $\rr^+$ en vez de $\nn$).
\lsep
El propósito del curso es estudiar un tipo especial pero muy importante de procesos estocásticos: los procesos de Markov.

\vs

En todo lo que sigue $I$\MarginNote{Espacio de Estados} (o a veces $E$) denotará un conjunto numerable, el ``espacio de estados'' donde nuestros procesos estocásticos tomarán valores.
Sobre $I$ consideramos la $\sigma$-álgebra de las partes $\cp(I)$.
Los elementos de $I$ los denotaremos típicamente por $i,j,k$ o $x,y,z$.
Detrás de todo lo que haremos habrá un espacio de probabilidad fijo $(\Omega,\cf,\pp)$, que solo invocaremos cuando sea necesario.

\begin{defn}\tbf{Cadena de Markov}
Un proceso estocástico $X=(X_n)_{n\geq0}$ a valores en $I$ es un \emph{proceso de Markov a tiempo discreto} o una \emph{cadena de Markov}\MarginNote{Cadena de Markov} si satisface la \emph{propiedad de Markov}:
\begin{equation}\label{eq:propMarkov}
\pp\big(X_{n+1}=i_{n+1}\big|X_0=i_0,\dotsc,X_{n}=i_{n}\big)=\pp\big(X_{n+1}=i_{n+1}\big|X_{n}=i_{n}\big)
\end{equation}\MarginNote{Propiedad de Markov}[-1cm]
para todo $i_0,\dotsc,i_{n+1}\in I$ (que satisfaga $\pp(X_0=i_0,\dotsc,X_{n}=i_{n}\big)>0$).
% Es decir, condicional al presente, el futuro del proceso es independiente del pasado.
\end{defn}

\begin{rem}
La propiedad de Markov es equivalente a decir que $X_{n+1}$ y \NAM[$(X_0,\dotsc,X_{n-1})$]{$(X_0,\dotsc,\\ X_{n-1})$} son independientes condicionales a $X_n$, es decir, ``condicional\MarginNote{Interpretación de la CM} al presente, el futuro del proceso es independiente del pasado''.
\uexer: demuestre esto.
\end{rem}


\begin{prop}
Si $X$ es una cadena de Markov entonces para todo $0\leq t_0\leq\dotsm\leq t_{n+1}$ en $\nn$ y todo $i_0,\dotsc,i_{n+1}\in I$,
\begin{equation}
\pp\big(X_{t_{n+1}}=i_{n+1}\big|X_{t_0}=i_0,\dotsc,X_{t_n}=i_{n}\big)=\pp\big(X_{t_{n+1}}=i_{n+1}\big|X_{t_n}=i_{n}\big).
\end{equation}
\end{prop}\MarginNote{CM a distintos tiempos}[-1cm]
\vskip
\begin{proof}
\uexer.
\end{proof}

\begin{ex}\tbf{Marcha aleatoria simple en $\zz$}\label{ex:marchaZ}\MarginNote{Marcha Aleatoria Simple en $\zz$}
Una partícula o individuo camina por $\zz$ de la siguiente forma.
A tiempo $0$ comienza en $X_0=x\in\zz$.
Luego en cada instante de tiempo decide dar un paso a la derecha con probabilidad $p\in[0,1]$ y un paso a la izquierda con probabilidad $1-p$, todo de manera independiente, es decir, para cada $n\geq1$ tenemos
\[X_n=\begin{dcases*}
X_{n-1}+1 & con probabilidad $p$,\\
X_{n-1}-1 & con probabilidad $1-p$.
\end{dcases*}\]
Entonces $(X_n)_{n\geq0}$ es una cadena de Markov (\uexers: chequee esto).
\lsep
Por ahora esta descripción es informal, luego veremos como se puede construir esta cadena de manera rigurosa.
\end{ex}

\begin{ex}\tbf{Ruina del jugador}\label{ex:ruina}
Consideramos ahora un juego de apuestas, donde en cada turno el jugador gana $1$ con probabilidad $p$ y pierde $1$ con probabilidad $1-p$.
El jugador para de jugar si se queda sin dinero.
Esto es muy similar a la cadena anterior, excepto que ahora el espacio de estados es $\zz_{\geq0}$ y tenemos
\[X_n=\begin{dcases*}
X_{n-1}+1 & con probabilidad $p$ si $n>0$,\\
X_{n-1}-1 & con probabilidad $1-p$ si $n>0$,\\
0 & con probabilidad $1$ si $n=0$.
\end{dcases*}\]
\end{ex}

\begin{ex}\tbf{Cadena de Ehrenfest}\label{ex:ehrenfest}
En este ejemplo nos imaginamos una caja rectangular dividida en dos cámaras que están conectadas por un pequeño orificio, donde $r$ moléculas de aire se mueven libremente al azar.
\begin{figure}[h]
\centering
\includegraphics[width=1.6in]{figures/ehrenfest.png}
\end{figure}
P. Ehrenfest propuso a comienzos del sigo XX el siguente modelo para estudiar esta situación.
Tenemos $r$ bolas distribuidas en 2 dos urnas, una con $k$ bolas (la cámara izquierda de la caja) y la otra con las restantes $r-k$ (la cámara derecha).
Luego en cada instante de tiempo tomamos una de las $r$ bolas elegida uniformemente al azar y la cambiamos de urna.
La cadena de Markov detrás de este ejemplo es la siguiente: $I=\{0,\dots,r\}$ y, para $n\geq1$, tenemos
\[X_n=\begin{dcases*}
X_{n-1}+1 & con probabilidad $\frac{r-X_{n-1}}{r}$,\\
X_{n-1}-1 & con probabilidad $\frac{X_{n-1}}{r}$.
\end{dcases*}\]
\end{ex}


\begin{defn}\tbf{Cadena de Markov homogénea, matriz de transición} 
Sea $X$ una cadena de Markov.
Decimos que $X$ es \emph{homogénea}\MarginNote{CM Homogénea} si
\[\pp(X_{m+1}=j|X_m=i)=\pp(X_{n+1}=j|X_n=i)\quad\forall~m,n\geq0,~i,j\in I.\]
En este caso a la matriz $P=\big(p_{ij}\big)_{i,j\in I}$\MarginNote{Matriz de Transición} dada por $p_{ij}=\pp(X_{1}=j|X_0=i)$ le llamamos \emph{matriz de transición} de la cadena de Markov $X$; $p_{ij}$\MarginNote{Prob. de Transición} es la \emph{probabilidad de transición de $i$ a $j$}.
\end{defn}

\begin{ex}
La marcha aleatoria simple del Ejem. \ref{ex:marchaZ} es una cadena de Markov homogénea, y su matriz de transición $P$ está dada por
\[p_{ij}=\begin{dcases*}
p & si $j=i+1$,\\
1-p & si $j=i-1$,\\
0 & cualquier otro $j$.
\end{dcases*}\]
\end{ex}

\begin{exer}
Escriba la matriz de transición asociada a la cadena de Ehrenfest introducida en el Ejemplo \ref{ex:ehrenfest}.
\end{exer}

Notar que $P$ es una matriz indexada por un conjunto numerable, pero esto no nos traerá mayores dificultades (lo discutiremos en más detalle más adelante).
$P$ no puede ser cualquier matriz, si no que claramente debe satisfacer algunas propiedades especiales, precisamente:

\begin{defn}\tbf{Matriz estocástica}
Una matriz $P=\big(p_{ij}\big)_{i,j\in I}$ se dice \MarginNote{Matriz Estocástica}\emph{matriz estocástica} si:\\[-20pt]
\begin{itemize}
\item $p_{ij}\geq0$ para todo $i,j\in I$.
\item $\sum_{j\in I}p_{ij}=1$ para todo $i\in I$.
\end{itemize}
En otras palabras, $P$ es una matriz estocástica si para cada $i\in I$, $(p_{ij})_{j\in I}$ es un vector de probabilidad.
\end{defn}

\begin{prop}\MarginNote{Matr. de Trans. de CM es Matr. Est.}
Si $P$ es la matriz de transición de una cadena de Markov, entonces $P$ es matriz estocástica.
\end{prop}

\vskip

\begin{proof} 
\uexer~(fácil).
\end{proof} 

Durante el curso estaremos interesados casi exclusivamente en el caso de cadenas de Markov homogéneas.
Por lo tanto, en todo lo que sigue trabajaremos bajo la siguiente convención:
\begin{quote}
% \begin{mdframed}[style=st_red]
\MarginNote{Todas las CM son homogéneas (convención)}Salvo en los casos puntuales en que se indique lo contrario, todas las cadenas de Markov que aparezcan en estas notas serán consideradas implícitamente como cadenas homogéneas.
% \end{mdframed}
\end{quote}

\vskip1pt

\begin{exer}
Sea $X$ una cadena de Markov no homogénea a valores en $I$.
Encuentre una cadena de Markov $\tilde X$ a valores en $I\times\nn$ que es homogénea y que permite recuperar la distribución de la cadena original $X$. 
\end{exer}

\subsubsection{Caracterización y construcción}

La matriz de transición gobierna la evolución de nuestra cadena de Markov $X$.
Para determinar la distribución de la cadena hace falta un ingrediente más:

\begin{defn}\tbf{Distribución inicial}\MarginNote{Distribución Inicial}
Al vector de probabilidad $\mu$ sobre $I$ definido por $\mu_i=\pp(X_0=i)$ le llamamos la \emph{distribución inicial} de la cadena de Markov $X$.
\end{defn}

Usaremos indistintamente la notación $\mu$ para un vector de probabilidad como en la definición y para una medida de probabilidad sobre $I$, que en cualquier caso son la misma cosa.

Es relativamente fácil ver que toda cadena de Markov, y en particular sus distribuciones finito-dimensionales, queda determinada por su matriz de transición y su distribución inicial:

\begin{prop}\label{prop:caractMarkov}\MarginNote{Caracterización de una Cadena de Markov}
$X$ es una cadena de Markov con matriz de transición $P$ y distribución inicial $\mu$ si y solo si
\[\pp(X_0=i_0,X_1=i_1,\dotsc,X_n=i_n)=\mu_{i_0}p_{i_0i_1}\dotsc p_{i_{n-1}i_n}.\]
\end{prop}

\begin{proof}
Para la primera implicancia escribimos
\NAM[
\begin{align}
\pp(X_0=i_0,\dotsc,X_n=i_n)
&=\pp(X_n=i_n|X_{n-1}=i_{n-1},\dotsc,X_0=i_0)\pp(X_{n-1}=i_{n-1},\dotsc,X_0=i_0)\hspace{-16pt}\\
&=\pp(X_n=i_n|X_{n-1}=i_{n-1})\pp(X_{n-1}=i_{n-1},\dotsc,X_0=i_0)\\
&=p_{i_{n-1}i_n}\pp(X_{n-1}=i_{n-1},\dotsc,X_0=i_0),\\
\shortintertext{donde en la segunda igualdad usamos la propiedad de Markov, y luego repetimos inductivamente hasta}
&=p_{i_{n-1}i_n}p_{i_{n-2}i_{n-1}}\dotsm p_{i_1i_0}\pp(X_0=i_0)\\
&=p_{i_{n-1}i_n}p_{i_{n-2}i_{n-1}}\dotsm p_{i_1i_0}\mu_{i_0},
\end{align}
]{
\begin{align}
\pp(X_0=i_0,\dotsc,X_n=i_n)
&=\pp(X_n=i_n|X_{n-1}=i_{n-1},\dotsc,X_0=i_0) \\
&\qquad \qquad \qquad \qquad \qquad \qquad \cdot \pp(X_{n-1}=i_{n-1},\dotsc,X_0=i_0)\hspace{-16pt}\\
&=\pp(X_n=i_n|X_{n-1}=i_{n-1})\pp(X_{n-1}=i_{n-1},\dotsc,X_0=i_0)\\
&=p_{i_{n-1}i_n}\pp(X_{n-1}=i_{n-1},\dotsc,X_0=i_0),\\
\shortintertext{donde en la segunda igualdad usamos la propiedad de Markov, y luego repetimos inductivamente hasta}
&=p_{i_{n-1}i_n}p_{i_{n-2}i_{n-1}}\dotsm p_{i_1i_0}\pp(X_0=i_0)\\
&=p_{i_{n-1}i_n}p_{i_{n-2}i_{n-1}}\dotsm p_{i_1i_0}\mu_{i_0},
\end{align}
}
que es lo que buscábamos.

Para la conversa, probemos primero que $X$ es cadena de Markov, es decir, que satisface la propiedad de Markov.
Usando la hipótesis tenemos
\begin{align}
\pp(X_{n+1}=i_{n+1}|X_{i_n},\dotsc,X_0=i_0)&=\frac{\pp(X_{n+1}=i_{n+1},\dotsc,X_0=i_0)}{\pp(X_{i_n},\dotsc,X_0=i_0)}\\
&=\frac{\mu_{i_0}p_{i_0i_1}\dotsc p_{i_{n}i_{n+1}}}{\mu_{i_0}p_{i_0i_1}\dotsc p_{i_{n-1}i_n}}=p_{i_{n}i_{n+1}}.
\end{align}
Por otro lado tenemos
\begin{align}
\pp(X_{n+1}=i_{n+1}|X_n=i_n)&=\frac{\pp(X_{n+1}=i_{n+1},X_n=i_n)}{\pp(X_n=i_n)}\\
&\textstyle=\frac1{\pp(X_n=i_n)}\sum_{i_0,\dotsc,i_{n-1}\in I}\pp(X_{n+1}=i_{n+1},\dotsc,X_0=i_0)\\
&\textstyle=\frac1{\pp(X_n=i_n)}\sum_{i_0,\dotsc,i_{n-1}\in I}\mu_{i_0}p_{i_0i_1}\dotsm p_{i_{n-1}i_{n}}p_{i_{n}i_{n+1}}\\&
\textstyle=\frac1{\pp(X_n=i_n)}\sum_{i_0,\dotsc,i_{n-1}\in I}\pp(X_0=i_0,\dotsc,X_{n}=i_{n})p_{i_{n}i_{n+1}},
\end{align}
donde usamos probabilidades totales y luego la hipótesis nuevamente.
Dejando el factor $p_{i_{n}i_{n+1}}$ afuera, la última suma vale $\pp(X_n=i_n)$, y entonces deducimos que $\pp(X_{n+1}=i_{n+1}|X_n=i_n)=p_{i_{n}i_{n+1}}$.
Juntando con lo anterior, esto nos da la propiedad de Markov, y además nos dice que $P$ es la matriz de transición de la cadena.
Falta derivar la condición inicial, pero esto es directo de la hipótesis, tomando $n=1$ y sumando sobre $i_1\in I$ (\uexers: hágalo).
\end{proof}

Es natural preguntarse ahora por la conversa al resultado anterior: dados un vector de probabilidad $\mu$ y una matriz estocástica $P$, existe una cadena de Markov con condición inicial $\mu$ y matriz de transición $P$?

\begin{thm}\tbf{Existencia de una cadena de Markov con $\mu$ y $P$ dados}\MarginNote{Exist. de una CM con $\mu$ y $P$ dados}
Sea $\mu$ una medida de probabilidad sobre $I$ y $P$ una matriz estocástica indexada por $I$.
Entonces existe un espacio de probabilidad $(\Omega,\cf,\pp)$ y una sucesión $X=\big(X_n\big)_{n\geq0}$ de variables aleatorias en este espacio tal que $X$ es una cadena de Markov con distribución inicial $\mu$ y matriz de transición $P$.
\end{thm}

La construcción de una cadena de Markov como en el teorema no es única.
En la demostración usaremos la que se conoce como la \emph{construcción canónica}, después veremos otra que también es importante.

\begin{sqgnote}
Repase el Teorema de Consistencia de Kolmogorov (ver, por ej., \cite{medidaJSM}).
\end{sqgnote}

\begin{proof}
Partimos eligiendo $\Omega$ como $I^\nn$ y $\cf=\cp(I)^\nn$ (la $\sigma$-álgebra producto); $(\Omega,\cf)$ elegidos así es el ``espacio medible canónico'' para la construcción de una cadena de Markov con $\mu$ y $P$ dados.
Para definir $\pp$ especificamos su valor sobre cilindros mediante
\begin{equation}\label{eq:cyl1}
\pp\big(\{\omega\in I^\nn\!:\omega_{0}=i_0,\dotsc,\omega_n=i_n\}\big)=\mu_{i_0}p_{i_0i_1}\dotsm p_{i_{n-1}i_n}.
\end{equation}
Usando el Teorema de Consistencia de Kolmogorov se tiene que existe una única medida de probabilidad $\pp$ sobre todo $I^\nn$ que satisface \eqref{eq:cyl1}.

\begin{exer}
Verificar esto.
\end{exer}

\noindent Ahora definimos la variable aleatoria $X_n\!:\Omega\longrightarrow I$ mediante
\[X_n(\omega)=\omega_n.\]
Por construcción se tiene entonces que
\begin{align}
\pp(X_0=i_0,\dotsc,X_n=i_n)&=\pp(\{\omega\in I^\nn\!:X_0(\omega)=i_0,\dotsc,X_n(\omega)=i_n)\\
&=\pp(\{\omega\in I^\nn\!:\omega_{0}=i_0,\dotsc,\omega_n=i_n\})=\mu_{i_0}p_{i_0i_1}\dotsm p_{i_{n-1}i_n}.
\end{align}
Gracias a la Prop. \ref{prop:caractMarkov} deducimos que $X=\big(X_n\big)_{n\geq0}$ satisface lo requerido.
\end{proof}

\begin{notation}\MarginNote{Notación para una dist. de una CM}
Escribiremos $X\sim\CM(\mu,P)$ si $X$ es cadena de Markov con distribución inicial $\mu$ y matriz de transición $P$.
Si no necesitamos especificar la distribución inicial, escribiremos $X\sim\CM(P)$.
\end{notation}

En la notación anterior no hacemos mención al espacio de probabilidad donde está definida la cadena.
En la mayoría de los resultados del curso esto no es relevante; los resultados valdrán para cadenas construidas en cualquier espacio de probabilidad si se asumen las hipótesis necesarias sobre su distribución (i.e. sobre $\mu$ y $P$).
Si uno lo desea, puede considerar entonces que todas nuestras cadenas están construidas usando la construcción canónica, salvo que se indique lo contrario.

Hay una construcción alternativa que es importante y muy útil, pues permite identificar muchos procesos estocásticos como cadenas de Markov (y demostrar que lo son) fácilmente.
La damos a continuación.

\begin{thm}\tbf{Construcción directa de cadenas de Markov}\MarginNote{Constr. directa de CM}
Sea $(\Omega,\cf,\pp)$ una espacio de probabilidad en el cual están definidas variables aleatorias independientes $(U_n)_{n\geq0}$ con distribución Unif$\ts[0,1]$ y adicionalmente una variable aleatoria $\xi_0$ a valores $I$ con distribución $\mu$, independiente de todas las $U_n$.
Sea $\Phi\!:\![0,1]\times I\longrightarrow I$ medible y definamos $\big(X_n\big)_{n\geq0}$ mediante
\begin{equation}
\begin{aligned}
X_0&=\xi_0,\\
X_{n+1}&=\Phi(U_n,X_n),\quad n\geq0.
\end{aligned}\label{eq:constrDirMarkov}
\end{equation}
Entonces $X\sim\CM(\mu,P)$ con $P$ dada por $p_{ij}=\pp(\Phi(U_0,i)=j)$.
\end{thm}

\begin{proof}
En clases. \ucmark
\end{proof}

\begin{rem}
\leavevmode
\begin{enumerate}
\item Toda cadena de Markov puede construirse de esta manera, es decir, si $\mu$ y $P$ están dados entonces podemos darnos un espacio de probabilidad como en el teorema y encontrar un $\Phi$ que hace lo requerido.
\lsep
En efecto, supongamos que nos dan $X\sim\CM(\mu,P)$ y consideremos un espacio de probabilidad con $\xi_0$ distribuido según $\mu$ (a valores en $I$) y una secuencia $(U_n)_{n\geq0}$ como en el teorema.
Debemos construir una función $\Phi$ que haga lo requerido.
Para esto enumeramos $I$ como $I=\{i_\ell\}_{\ell\geq0}$ y definimos, para cada $k,\ell\in\nn$,
\[a^{k}_\ell=\sum_{j=0}^\ell p_{i_ki_j}\quad\uptext{(que es }=\pp(X_1\in\{i_0,\dotsc,i_\ell\}|X_0=i_k)\uptext{)},\]
y además $a^k_{-1}=0$.
Los intervalos $\{[a^k_{\ell-1},a^k_\ell),\,\ell\geq0\}$, que son fijos (y deterministas) en función de $P$, particionan $[0,1]$, luego podemos definir $\Phi(u,i_k)=i_\ell$ donde $\ell$ es el único índice tal que $u\in[a^k_{\ell-1},a^k_\ell)$.
Con esto tenemos
\[\pp(\Phi(U_0,i_k)=i_\ell)=\pp(U_0\in[a^k_{\ell-1},a^k_\ell))=a^k_\ell-a^k_{\ell-1}=p_{i_ki_\ell},\]
tal como requeríamos.
\item La conclusión del teorema sigue valiendo si cambiamos las $U_n$ por cualquier otra sucesión de variables aleatorias i.i.d. (pero en ese caso no necesariamente vale el punto 1 anterior).
\item La construcción puede pensarse como una versión para cadenas de Markov del siguiente resultado clásico de probabilidades: si $F$ es  la función de distribución (acumulada) de una variable aleatoria real, y si definimos $G\!:[0,1]\longrightarrow\rr$ mediante la fórmula $G(a)=\sup\{x\in\rr\!:F(x)<a)$, entonces si $U\sim$~Unif[0,1], se tiene que la variable aleatoria $G(U)$ tiene función de distribución $F$ ($G$ en este caso puede pensarse como una especie de inversa de la función inyectiva, pero no necesariamente biyectiva, $F$).
\lsep 
En particular, la construcción entrega una manera de extender este último resultado a variables aleatorias a valores en conjuntos numerables.
\end{enumerate}
\end{rem}

\begin{exer}
Demuestre el resultado mencionado en (iii) de la observación anterior y, en particular, que $G$ está bien definida (y coincide con $F^{-1}$ si $F$ es invertible).
\end{exer}

Una de las gracias del teorema anterior es que nos permite chequear fácilmente que procesos estocásticos concretos satisfacen la propiedad de Markov.

\begin{ex}
Volvemos a la marcha aleatoria simple del Ejem. \ref{ex:marchaZ}.
Aquí podemos tomar $\xi_0=x$ y definir
\[\Phi(u,i)=\begin{dcases*}
i+1 & si $u\leq p$,\\
i-1 & si $u>p$.
\end{dcases*}\]
El teorema nos asegura que el proceso estocástico construido de acuerdo a \eqref{eq:constrDirMarkov} es una cadena de Markov con matriz de transición requerida.
\end{ex}

\subsubsection{Probabilidades de transición}

\begin{notation}
Si $X\sim\CM(P)$ e $i\in I$ usaremos denotaremos por $\pp_i$\MarginNote{Notación $\pp_i$} a la medida de probabilidad correspondiente a la cadena $X\sim\CM(\delta_i,P)$ con $\delta_i(j)=\uno{i=j}$ (es decir, a la cadena que parte en $i$).
Usaremos $\ee_i$\MarginNote{Notación $\ee_i$}[-2pt] para denotar la esperanza bajo $\pp_i$.
\lsep
Más generalmente, si $\mu$ es una distribución de probabilidad sobre $I$, usamos \MarginNote{Notación $\pp_\mu$ y $\ee_\mu$}$\pp_\mu$ (y $\ee_\mu$) para denotar la distribución de la cadena $X\sim\CM(\mu,P)$.
\end{notation}

\begin{exer}
Convencerse de que $\pp_\mu=\sum_{i\in I}\mu_i\pp_i$ y $\ee_\mu=\sum_{i\in I}\mu_i\ee_i$.
\end{exer}

Como ya mencionamos, $P$ es una matriz indexada por el conjunto numerable $I$.
A menudo es útil \MarginNote{Interpretación de $P$ como operador lineal}interpretar a $P$ como un operador lineal que actúa sobre funciones $f\!:I\longrightarrow I$ de acuerdo a 
\begin{equation}
Pf(i)=\sum_{j\in I}p_{ij}f(j).\label{eq:Pf}
\end{equation}
La suma podría ser o no convergente, pero en muchos casos de interés lo es; por ejemplo si $f$ es acotada (por Teorema de Convergencia Dominada) o positiva (por Teorema de Convergencia Monótona, aunque en este caso debemos aceptar que la suma valga $\infty$).
Cuando $I$ es finito, lo anterior corresponde exactamente a la acción de $P$ como matriz indexada por $I\times I$ al multiplicar a $f$ como vector indexado por $I$, y esta perspectiva podemos extenderla naturalmente al caso en que $I$ que infinito.
\lsep
En particular, podemos definir\MarginNote{Producto de Matr. Est.} productos de matrices estocásticas mediante la fórmula usual, $(PQ)_{ij}=\sum_{k\in I}p_{ik}q_{kj}$, que en este caso siempre es convergente.
Naturalmente denotamos también por $P^0$ a la matriz identidad, es decir $(P^0)_{ij}=\uno{i=j}$.
\lsep
La fórmula \eqref{eq:Pf} tiene una\MarginNote{Interpr. de la ec. \eqref{eq:Pf}} interpretación probabilística natural:
\[Pf(i)=\ee_{i}(f(X_1))\]
De manera similar podemos multiplicar a un vector de probabilidad $\mu$ sobre $I$ por la derecha, y obtenemos
\[(\mu P)_j=\sum_{i\in I}\mu_ip_{ij}=\pp_\mu(X_1=j).\]
\uexer: Convénzase de las dos fórmulas anteriores.

\begin{defn}\tbf{Probabilidades de transición}
Dada $X\sim\CM(P)$ y $n\in\nn$, denotamos por $P^{(n)}=(p^{(n)}_{ij})_{i,j\in I}$ a la\MarginNote{Matriz de Transición en $n$ pasos} \emph{matriz de transición en $n$ pasos} dada por
\[p^{(n)}_{ij}=\pp_i(X_n=j).\]
\end{defn}

\begin{thm}\tbf{Ecuaciones de Chapman-Kolmogorov}\MarginNote{Ec. de Chapman-Kolmogorov}
Si $X\sim\CM(P)$ entonces para todo $m,n\geq0$ se tiene
\[p^{(m+n)}_{ij}=\sum_{k\in I}p^{(m)}_{ik}p^{(n)}_{kj}.\]
En particular, \MarginNote{$P^{(n)}=P^n$}
\[P^{(n)}=P^n\] 
para todo $n\geq0$.
\end{thm}

\begin{proof}
\NAM[
\begin{align}
p^{(m+n)}_{ij}&=\pp_i(X_{m+n}=j)=\sum_{k\in I}\pp_i(X_{m+n}=j|X_m=k)\pp_i(X_m=k)\\
&=\sum_{k\in I}\pp(X_{m+n}=j|X_0=i,X_m=k)\pp_i(X_m=k)=\sum_{k\in I}\pp(X_{m+n}=j|X_m=k)\pp_i(X_m=k)\hspace{-18pt}\\
&=\sum_{k\in I}\pp_k(X_{n}=j)\pp_i(X_m=k)=\sum_{k\in I}p^{(m)}_{ik}p^{(n)}_{kj}.
\end{align}
]{
\begin{align}
p^{(m+n)}_{ij}
&=\pp_i(X_{m+n}=j)
=\sum_{k\in I}\pp_i(X_{m+n}=j|X_m=k)\pp_i(X_m=k)\\
&=\sum_{k\in I}\pp(X_{m+n}=j|X_0=i,X_m=k)\pp_i(X_m=k)\\
&=\sum_{k\in I}\pp(X_{m+n}=j|X_m=k)\pp_i(X_m=k)\hspace{-18pt}\\
&=\sum_{k\in I}\pp_k(X_{n}=j)\pp_i(X_m=k)=\sum_{k\in I}p^{(m)}_{ik}p^{(n)}_{kj}.
\end{align}
}
La segunda afirmación sale de la primera por inducción, y además claramente implica la primera por definición de potencia de matrices.
\end{proof}

\subsubsection{Versión general de la propiedad de Markov}

\begin{defn}\tbf{Filtración}
Sea $(\Omega,\cf)$ un espacio medible.
Una\MarginNote{Filtración} \emph{filtración} en este espacio es una familia $\big(\cf_n\big)_{n\geq0}$ de sub $\sigma$-álgebras de $\cf$ tal que $\cf_n\subseteq\cf_{n+1}$ para cada $n\geq0$.
\lsep
Si $X$ es un proceso estocástico construido en un espacio de probabilidad $(\Omega,\cf,\pp)$, la \emph{filtración natural}\MarginNote{Filtración Natural} asociada a $X$ es aquella dada por
\[\cf_n=\sigma(\{X_0,\dotsc,X_n\}).\]
\end{defn}

La\MarginNote{Filtración natural es la más pequeña} filtración natural es aquella filtración más pequeña que tal que $X_0,\dotsc,X_n$ es $\cf_n$-medible para cada $n\geq0$, en el sentido de que para cualquier otra filtración $\big(\cg_n\big)_{n\geq0}$ que satisfaga lo mismo necesariamente debe tenerse que $\cg_n\supseteq\cf_n$ para cada $n\geq0$.
\begin{quote}
Durante todo el curso, $\big(\cf_n\big)_{n\geq0}$ denotará la filtración natural asociada al proceso estocástico que se esté considerando.
\end{quote}
\lsep
$\cf_n$\MarginNote{$\cf_n$ representa la información contenida} nos permite representar la información contenida en la evolución del proceso hasta tiempo $n$.

\begin{notation}
Si $\cg$ es sub $\sigma$-álgebra de $\cf$ y $A\in\cf$, escribimos \MarginNote{Notación de $\pp(A|\cg)$}
\[\pp(A|\cg)=\ee(\uno{A}|\cg).\]
\end{notation}

En el siguiente resultado reinterpretamos la definición y caracterización de cadenas de Markov en el lenguaje de esperanzas condicionales:

\begin{prop}\MarginNote{Caract. de CM usando filtración natural}
Sea $X$ un proceso estocástico (y $\big(\cf_n\big)_{n\geq0}$ su filtración natural asociada).
Son equivalentes:
\begin{enumerate}[label=\uptext{(\roman*)}]
\item $X$ es cadena de Markov (homogénea).
\item $\pp(X_{n+1}=j|\cf_n)=\sum_{i\in I}\pp(X_{n+1}=j|X_n=i)\uno{X_n=i}$.
\item $\pp(X_{n+1}=j|\cf_n)$ es $\sigma(\{X_n\})$-medible.
\end{enumerate}	
En particular, si $X\sim\CM(P)$ se tiene que
\[\pp(X_{n+1}=j|\cf_n)(\omega)=p_{X_n(\omega),j}\]
(donde para mayor claridad hemos incluido los $\omega$'s, aunque casi nunca los escribiremos).
\end{prop}

\begin{rem}\label{rem:abusonot}\MarginNote{Abuso de notación de la igualdad anterior}
A veces abusaremos notación y escribiremos la última igualdad de la siguiente forma:
\[\pp(X_{n+1}=j|\cf_n)(\omega)=\pp_{X_n(\omega)}(X_1=j).\]
En estos casos, el lado derecho debe interpretarse como $\pp_{X_n(\omega)}(\tilde X_1=j)$ donde $\tilde X$ es una copia independiente de $X$.
\end{rem}

\begin{proof}
Tenemos que $\cf_n=\sigma(\{X_0,\dotsc,X_n\})=\sigma\big(\big\{\bigcap_{j=0}^nX_j^{-1}(\{i_j\}),\,i_0,\dotsc,i_n\in I\big\}\big)$.
La lista de conjuntos que aparece generando la $\sigma$-álgebra es una partición de $\Omega$ y luego, gracias a la Prop. \ref{prop:totProbEst},
\begin{align}
\pp(X_{n+1}=j|\cf_n)&=\sum_{i_0,\dotsc,i_n\in I}\frac{\pp(X_0=i_0,\dotsc,X_n=i_n,X_{n+1}=j)}{\pp(X_0=i_0,\dotsc,X_n=i_n)}\uno{X_0=i_0,\dotsc,X_n=i_n}\\
&=\sum_{i_0,\dotsc,i_n\in I}\pp(X_{n+1}=j|X_0=i_0,\dotsc,X_n=i_n)\uno{X_0=i_0,\dotsc,X_n=i_n}.
\end{align}
El lado derecho es igual a $\sum_{i_n\in I}\pp(X_{n+1}=j|X_n=i_n)\uno{X_n=i_n}$ si y sólo si $\pp(X_{n+1}=j|X_0=i_0,\dotsc,X_n=i_n)=\pp(X_{n+1}=j|X_n=i_n)$ (\uexers: demuestre esta equivalencia), que es justamente la propiedad de Markov.
Esto nos da la equivalencia de (i) y (ii).

Que (ii) implica (iii) es directo.
Para la conversa usamos la hipótesis de medibilidad para escribir
\begin{align}
\pp(X_{n+1}=j|\cf_n)&=\ee(\pp(X_{n+1}=j|\cf_n)|\sigma(\{X_n\}))=\ee(\ee(\uno{X_{n+1}=j}|\cf_n)|\sigma(\{X_n\}))\\
&=\ee(\uno{X_{n+1}=j}|\sigma(\{X_n\})).
\end{align}
Desarrollando la última esperanza condicional como en la primera equivalencia deducimos que (ii) se tiene.
\end{proof}

La siguiente es una versión general de la propiedad de Markov.

\begin{thm}\label{thm:propMarkGen}\MarginNote{Vers. general de la propiedad de Markov}
Sea $X$ cadena de Markov (homogénea) y sea $f\!:I^\nn\longrightarrow\rr$ medible y acotada.
Entonces
\[\ee\big(f(X_{n+1},X_{n+2}\dotsc)\big|\cf_n\big)=\ee_{X_n}\tsm\big(f(X_1,X_2,\dotsc)\big).\]
\end{thm}

En palabras simples, la propiedad de Markov dice que:
\begin{quote}
\emph{Si reiniciamos la cadena a tiempo $n$ y observamos su trayectoria a partir de ese tiempo, lo que vemos es una copia de la cadena original, partiendo desde el estado $X_n$, pero independiente de los estados $X_0,\dotsc,X_{n-1}$.}
\end{quote}

\begin{rem}
\leavevmode
\begin{enumerate}[label=(\roman*)]
\item Lo anterior es una equivalencia (\uexers{}: demuéstrelo).
\item Acá hemos abusado notación nuevamente, ver la Obs. \ref{rem:abusonot}, el significado de lo que escribimos es análogo.
\item Se tiene también que $\ee\big(f(X_{n},X_{n+1},\dotsc)\big|\cf_n\big)=\ee_{X_n}\tsm\big(f(X_0,X_1,\dotsc)\big)$, la demostración es la misma (o puede obtenerse como corolario).
\end{enumerate}
\end{rem}

\begin{proof}
Partiremos probando primero por inducción que la propiedad se tiene para el conjunto ${\cal R}$ de funciones de la forma $f(i_1,i_2,\dotsc)=f_1(i_1)f_2(i_2)\dotsm f_m(i_m)$, donde $m\geq1$ y $f_1,\dotsc,f_m\!:I\longrightarrow\rr$ son medibles y acotadas.

\begin{exer}
Use la propiedad de Markov para probar el caso base, $m=1$.
\end{exer}

Ahora hacemos el paso inductivo, considerando el caso $m+1$.
Tenemos
\NAM[
\begin{align}
\ee\big(f(X_{n+1},X_{n+2},\dotsc)\big|\cf_n\big)&=\ee\big(f_1(X_{n+1})f_2(X_{n+2})\dotsm f_{m+1}(X_{n+m+1})\big|\cf_n\big)\\
&=\ee\big(\ee\big(f_1(X_{n+1})f_2(X_{n+2})\dotsm f_{m+1}(X_{n+m+1})\big|\cf_{n+m}\big)\big|\cf_n\big)\\
&=\ee\big(f_1(X_{n+1})f_2(X_{n+2})\dotsm f_{m}(X_{n+m})\ee\big(f_{m+1}(X_{m+1})\big|\cf_{n+m}\big)\big|\cf_n\big)\hskip-15pt\\
&=\ee\big(f_1(X_{n+1})f_2(X_{n+2})\dotsm f_{m}(X_{n+m})\ee_{X_{n+m}}\big(f_{m+1}(X_1)\big)\big|\cf_n\big),
\end{align}
]{
\begin{align}
\ee\big(f(X_{n+1},X_{n+2},& \dotsc)\big |\cf_n\big)\\
&=\ee\big(f_1(X_{n+1})f_2(X_{n+2})\dotsm f_{m+1}(X_{n+m+1})\big|\cf_n\big)\\
&=\ee\big(\ee\big(f_1(X_{n+1})f_2(X_{n+2})\dotsm f_{m+1}(X_{n+m+1})\big|\cf_{n+m}\big)\big|\cf_n\big)\\
&=\ee\big(f_1(X_{n+1})f_2(X_{n+2})\dotsm f_{m}(X_{n+m})\ee\big(f_{m+1}(X_{m+1})\big|\cf_{n+m}\big)\big|\cf_n\big)\hskip-15pt\\
&=\ee\big(f_1(X_{n+1})f_2(X_{n+2})\dotsm f_{m}(X_{n+m})\ee_{X_{n+m}}\big(f_{m+1}(X_1)\big)\big|\cf_n\big),
\end{align}
}
donde usamos el caso base en la última igualdad.
Notemos que el último factor dentro de la esperanza depende ahora solo de $X_{n+m}$, al igual que el que lo precede.
Entonces si definimos $\tilde f_m(j)=f_m(j)\ee_{j}\big(f_1(X_1)\big)$ y usamos la hipótesis inductiva, lo anterior es igual a
\begin{align}
&\ee\big(f_1(X_{n+1})f_2(X_{n+2})\dotsm \tilde f_{m}(X_{n+m})\big|\cf_n\big)
=\ee_{X_n}\big(f_1(X_{1})f_2(X_{2})\dotsm \tilde f_{m}(X_{m})\big)\\
&\qquad=\ee_{X_n}\big(f_1(X_{1})f_2(X_{2})\dotsm f_{m}(X_{m})\ee_{X_{m}}\big(f_{m+1}(X_1)\big)\big)\\
&\qquad=\ee_{X_n}\big(f_1(X_{1})f_2(X_{2})\dotsm f_{m}(X_{m})\ee\big(f_{m+1}(X_{m+1})\big|\cf_{n+m}\big)\big)\\
&\qquad=\ee_{X_n}\big(\ee\big(f_1(X_{1})f_2(X_{2})\dotsm f_{m}(X_{m})f_{m+1}(X_{m+1})\big|\cf_{n+m}\big)\big)\\
&\qquad=\ee_{X_n}\big(f_1(X_{1})f_2(X_{2})\dotsm f_{m+1}(X_{m+1})\big),
\end{align}
que es lo que queríamos.

Ahora concluimos.
Sea ${\cal A}$ el conjunto de funciones $f\!:I^\nn\longrightarrow\rr$ medibles y acotadas que satisfacen la igualdad que queremos probar.
${\cal A}$ claramente es un espacio vectorial, contiene a las constantes, y por Teorema de la Clase Monótona, si $(f_n)_{n\geq0}\subset{\cal A}$ con $f_n\geq0$ satisface $f_n\nearrow f$ entonces $f\in{\cal A}$.
Arriba probamos que ${\cal R}\subseteq{\cal A}$, y es evidente que ${\cal R}$ es cerrado bajo producto, y luego por la forma funcional del Teorema de la Clase Monótona, $\sigma({\cal R})\subseteq{\cal A}$.
Pero $\sigma({\cal R})$ es claramente todo el espacio de funciones medibles y acotadas de $I^\nn$ en $\rr$, lo que termina la demostración.
\end{proof}

\subsection{Propiedad de Markov fuerte}

Nuestro objetivo ahora es extender la propiedad de Markov a tiempos aleatorios.
Lo primero es definir y estudiar una clase adecuada de tiempos aleatorios.
intuitivamente, si queremos que la propiedad de Markov siga valiendo, lo que necesitamos es que estos tiempos solo dependan de la trayectoria del proceso hasta el presente, pero no de su futuro.
A continuación veremos la manera de hacer esto preciso.

\subsubsection{Tiempos de parada}

\begin{defn}\tbf{Tiempos de parada} \MarginNote{Tiempos de Parada}
Sea $(\Omega,\cf,\pp,(\cf_n)_{n\geq0})$ un \emph{espacio de probabilidad filtrado}, es decir, un espacio de probabilidad $(\Omega,\cf,\pp)$ con una filtración $(\cf_n)_{n\geq0}$ definida sobre él.
Decimos que una variable aleatoria $\tau\!:\Omega\longrightarrow\nn\cup\{\infty\}$ es un \emph{tiempo de parada} si
\[\{\tau\leq n\}\in\cf_n\quad\forall~n\geq0.\]
Abreviaremos tiempo de parada como t.d.p.
\end{defn}

\begin{ex}\label{ex:taua}
Si $X$ es una marcha aleatoria simple y $a\in\zz$, $\tau_a=\inf\{m\geq0\!:X_m\geq a\}$ es un t.d.p.
\end{ex}

\begin{exer}
Demuestre que la propiedad que define t.d.p. es equivalente a pedir que $\{\tau=n\}\in\cf_n$ para todo $n\geq0$.
\end{exer}

\begin{prop}\MarginNote{Prop. de tiempos de parada}%\tbf{Propiedades de tiempos de parada}
\leavevmode
\begin{enumerate}[label=\uptext{(\arabic*)}]
\item Si $\tau_1,\tau_2$ son t.d.p. entonces $\tau_1\wedge\tau_2\coloneqq\min\{\tau_1,\tau_2\}$ y $\tau_1\vee\tau_2\coloneqq\max\{\tau_1,\tau_2\}$ son t.d.p.
\item Si $(\tau_n)_{n\geq0}$ son t.d.p. entonces $\liminf_n\tau_n$, $\limsup_n\tau_n$, $\inf_n\tau_n$ y $\sup_n\tau_n$ son t.d.p.
\end{enumerate}
\end{prop}

\begin{proof}
\uexer.
\end{proof}

Dada una filtración $(\cf_n)_{n\geq0}$, escribiremos\MarginNote{Notación de $\cf_\infty$}
\[\cf_\infty=\lim_{n\to\infty}\cf_n=\bigcup_{n\geq0}\cf_n.\]
En el caso de la filtración natural asociada a un proceso estocástico, esto\MarginNote{Intuición sobre $\cf_\infty$} corresponde intuitivamente a la información contenida en la trayectoria completa del proceso.

\begin{defn}
Dado un espacio de probabilidad filtrado y un t.d.p. $\tau$,\MarginNote{Def. de $\cf_\tau$} definimos
\[\cf_\tau=\big\{A\in\cf_\infty\!:A\cap\{\tau\leq n\}\in\cf_n~\forall n\geq0\}.\]
\end{defn}

\begin{ex}
Probar que si $\tau=n$, entonces con la definición anterior se cumple $\cf_\tau=\cf_n$.
\end{ex}

\begin{prop}\MarginNote{\sffamily Propiedades de $\cf_\tau$}\tbf{Propiedades de $\cf_\tau$}
Sean $\tau$, $\tau_1$, $\tau_2$ t.d.p.
\begin{enumerate}[label=\uptext{(\arabic*)}]
\item $\cf_\tau$ es $\sigma$-álgebra.
\item $\tau$ es $\cf_\tau$-medible.
\item Si $\tau_1(\omega)\leq\tau_2(\omega)$ para cada $\omega\in\Omega$ entonces $\cf_{\tau_1}\subseteq\cf_{\tau_2}$.
\item $\cf_\tau=\big\{A\in\cf_\infty\!:A\cap\{\tau= n\}\in\cf_n~\forall n\geq0\}$.
\item $A\in\cf_\tau$ si y sólo si existe $(A_n)_{n\in\nn\cup\{\infty\}}$ con $A_n\in\cf_n$ tal que $A=\bigcup_{n\in\nn\cup\{\infty\}}A_n\cap\{\tau=n\}$.
\item $\{\tau_1\leq\tau_2\},\{\tau_1<\tau_2\}\in\cf_{\tau_1}\cap\cf_{\tau_2}$.
\end{enumerate}
\end{prop}

\begin{proof}
Consideremos (1).
Claramente $\emptyset,\Omega\in\cf_\tau$.
Ahora supongamos que $A\in\cf_\tau$.
Como $A\cap\{\tau\leq n\}\in\cf_n$ para cada $n$, tomando complemento tenemos $A^{\sf c}\cup\{\tau>n\}\in\cf_n$ para cada $n$.
Pero además $\{\tau\leq n\}\in\cf_n$, y entonces $\{\tau\leq n\}\cap\big(A^{\sf c}\cup\{\tau>n\}\big)=A^{\sf c}\cap\{\tau\leq n\}\in\cf_n$.
Luego $A^{\sf c}\in\cf_\tau$.
\uexer: verificar la propiedad de uniones numerables para terminar la demostración de (1).
\uexer: Demostrar (2), (3) y (4).

Para demostrar (5) tomamos $A\in\cf_\tau$ y escribimos
\[A=\bigcup_{n\in\nn\cup\{\infty\}}A\cap\{\tau=n\}=\bigcup_{n\in\nn\cup\{\infty\}}A_n\cap\{\tau=n\}\]
con $A_n=A\cap\{\tau=n\}$.
La otra implicancia es aún más directa.
\end{proof}

Terminamos esta parte con un resultado que será útil a continuación, y que es muy similar a la propiedad (5) anterior y se demuestra de manera análoga (\uexers: hágalo).

\begin{lem}\label{lem:Ymed}\MarginNote{Caract. de que una VA sea $\cf_\tau$-med.}
$Y\!:\Omega\longrightarrow\rr$ es $\cf_\tau$-medible si y sólo si existe una sucesión $(Y_n)_{n\in\nn\cup\{\infty\}}$ de variables aleatorias, con $Y_n$ $\cf_n$-medible, tal que $Y=\sum_{n\in\nn\cup\{\infty\}}Y_n\uno{\tau=n}$.
\end{lem}

\subsubsection{Propiedad de Markov fuerte}

Con las definiciones y resultados anteriores ya estamos en condiciones de establecer la versión esencialmente más fuerte de la propiedad de Markov disponible para cadenas de Markov.

\begin{thm}\tbf{Propiedad de Markov fuerte}\MarginNote{\text{Prop. de Markov Fuerte}}
Sea $X$ cadena de Markov (homogénea) y sea $f\!:I^\nn\longrightarrow\rr$ medible y acotada.
Sea $\tau$ un tiempo de parada.
Entonces
\begin{equation}
\ee\big(f(X_{\tau},X_{\tau+1}\dotsc)\big|\cf_\tau\big)\uno{\tau<\infty}=\ee_{X_\tau}\tsm\big(f(X_0,X_1,\dotsc)\big)\uno{\tau<\infty}.\label{eq:propMarkF}
\end{equation}
\end{thm}

Esto es, la misma propiedad demostrada en el Teo. \ref{thm:propMarkGen} (y todas las que le preceden, que son casos particulares de ella) valen también en el caso de tiempos de parada $\tau$, en el evento en que $\tau$ es finito (la multiplicación por $\uno{\tau<\infty}$ significa que la igualdad de las dos esperanzas se establece para $\omega\in\Omega$ tal que $\tau(\omega)<\infty$; en caso contrario las indicatrices hacen que la igualdad sea trivial).

\begin{proof}
La idea es reducirnos a la propiedad Markov ya conocida.
Sea $\varphi\!:I\longrightarrow\rr$ dada por $\varphi(i)=\ee_i(f(X_0,X_1,\dotsc))$.
Como $f$ es $\cp(I^\nn)$-medible y $X_n$ es $\cf_n$-medible, $\varphi(X_n)$ es $\cf_n$-medible.
Luego gracias al Lem. \ref{lem:Ymed} tenemos que $\uno{\tau<\infty}\varphi(X_\tau)$, que es igual a $\sum_{n\geq0}\uno{\tau=n}\varphi(X_n)$, es $\cf_\tau$-medible.

En la expresión del lado izquierdo de \eqref{eq:propMarkF} podemos mover la indicatriz $\uno{\tau<\infty}$ adentro de la esperanza condicional (pues $\{\tau<\infty\}\in\cf_\tau$).
Luego la propiedad que queremos probar es equivalente, por definición de la esperanza condicional, a pedir que para todo $A\in\cf_\tau$
\[\ee\big(f(X_{\tau},X_{\tau+1}\dotsc)\uno{A}\uno{\tau<\infty}\big)=\ee\big(\ee_{X_\tau}\tsm\big(f(X_0,X_1,\dotsc)\big)\uno{A}\uno{\tau<\infty}\big).\]
El lado derecho es igual a $\ee\big(\varphi(X_\tau)\uno{A}\uno{\tau<\infty}\big)$, y luego descomponiendo de acuerdo al valor de $\tau$, lo anterior es lo mismo que probar que 
\[\ee\big(f(X_{n},X_{n+1}\dotsc)\uno{A}\uno{\tau=n}\big)=\ee\big(\ee_{X_n}\tsm\big(f(X_0,X_1,\dotsc)\big)\uno{A}\uno{\tau=n}\big)\]
para cada $A\in\cf_\tau$ y cada $n\geq0$, gracias a que $f$ es acotada y el Teorema de Convergencia Dominada.
Pero $A\cap\{\tau=n\}\in\cf_n$, y entonces por definición de esperanza condicional nuevamente y la propiedad de Markov usual (Teo. \ref{thm:propMarkGen}) la igualdad anterior se tiene, lo que termina la demostración.
\end{proof}

\begin{rem}
Hemos escrito la propiedad de Markov para $f$ acotada por simplicidad, pero la demostración puede adaptarse a otros casos, por ejemplo $f\geq0$ usando Teorema de Convergencia Monótona.
\end{rem}

\begin{ex}
La hipótesis de que $\tau$ sea t.d.p. es necesaria para que una propiedad como la del teorema anterior sea válida.
Por ejemplo podemos considerar la marcha aleatoria simple en $\zz$ del Ej. \ref{ex:marchaZ} partiendo desde $0$ y definir el tiempo $\sigma=\sup\{k\geq0\!:X_k\leq0\}$, que claramente depende del futuro de la trayectoria (pues necesita saber que la marcha nunca más será negativa), por lo que no es tiempo de parada.
Si asumimos $\sigma<\infty$ y observamos la trayectoria de marcha a partir de tiempo $\sigma$, donde tenemos necesariamente $X_\sigma=0$, lo que vemos no es una copia independiente de la cadena comenzada en $0$, pues si a tiempo $\sigma$ es la última vez que estuvimos en $\zz_{\leq0}$ entonces el resto de la trayectoria debe mantenerse sobre $0$ (por ejemplo, $X_{\sigma+1}=1$).
% Como la marcha parte en $0$ se tiene que si $\sigma<\infty$ entonces $X_\sigma=0$ (recordando que la marcha solo se mueve con pasos de largo $\pm1$).
% Si la propiedad de Markov se tuviera, entonces por ejemplo tendríamos $\ee(X_{\sigma+1}|\cf_\sigma)\uno{\sigma<\infty}=\ee_{X_\sigma}(X_{1})\uno{\sigma<\infty}=\ee_{0}(X_{1})\uno{\sigma<\infty}=(p-q)\uno{\sigma<\infty}$.
% Pero claramente $\ee(X_{\sigma+1}|\cf_\sigma)\uno{\sigma<\infty}=\uno{\sigma<\infty}$, porque si a tiempo $\sigma$ es la última vez que estuvimos en $\zz_{\leq0}$, entonces un instante de tiempo después necesariamente estaremos en $1$.
% Como $p-q<1$ si $p<1$, vemos que esto no es posible.
\end{ex}

\subsubsection{Ejemplo: probabilidades de pasada}\label{sec:pasada}

La propiedad de Markov fuerte es una de las herramientas más importantes en el estudio de cadenas de Markov.
Para ilustrar esto veremos un\MarginNote{Ej. de Prop. de Markov Fuerte en Marcha Aleat.} ejemplo.
Consideremos de nuevo la marcha aleatoria simple en $\zz$ del Ej. \ref{ex:marchaZ} partiendo desde $0$ y definamos el siguiente tiempo de parada (ya introducido en el Ej. \ref{ex:taua}): para $a>0$ dado,
\[\tau_{a}=\inf\{n\geq0\!:X_n\geq a\},\]
es decir, el primer tiempo en que la marcha alcanza el nivel $a$.
La pregunta que queremos responder es la siguiente: ¿cuál es la probabilidad de que la cadena llegue a $a$ alguna vez, es decir, del evento $\{\tau_a<\infty\}$?

El truco es considerar la función generadora de momentos de $\tau_a$: para $s\in(0,1)$ definimos
\begin{equation}\label{eq:phia}
\phi_a(s)=\ee_0(s^{\tau_a})=\sum_{k\geq0}s^k\pp_0(\tau_a=k).
\end{equation}
Partimos escribiendo
\[\phi_a(s)=\ee_0(s^{\tau_a}\uno{\tau_1<\infty})+\ee_0(s^{\tau_a}\uno{\tau_1=\infty})=\ee_0(s^{\tau_a}\uno{\tau_1<\infty}),\]
pues si la marcha nunca alcanza el nivel $1$ (que es lo que indica el evento $\{\tau_1=\infty\}$) entonces tampoco podrá alcanzar el nivel $a\geq1$, lo que implica $\tau_a=\infty$ y luego $s^{\tau_a}=0$.
Ahora usando propiedades de la esperanza condicional tenemos
\[\phi_a(s)=\ee_0(\ee(s^{\tau_a}\uno{\tau_1<\infty}|\cf_{\tau_1}))=\ee_0(\ee(s^{\tau_a}|\cf_{\tau_1})\uno{\tau_1<\infty})=\ee_0(\ee(s^{\tau_a-\tau_1}|\cf_{\tau_1})s^{\tau_1}\uno{\tau_1<\infty}).\]
Ahora notamos dos cosas: 1. $s^{\tau_a-\tau_1}$ es acotado (por $1$, porque $\tau_a\geq\tau_1$), y 2. la variable aleatoria $\tau_a-\tau_1$ solo depende de la trayectoria de la marcha aleatoria a partir de tiempo $\tau_1$ (pues es el tiempo que le toma llegar al nivel $a$ luego de haber alcanzado el nivel $1$, que sucede precisamente a tiempo $\tau_11$).
Luego podemos aplicar la propiedad de Markov fuerte al lado derecho anterior para deducir que es igual a
\[\ee_0(\ee_{X_{\tau_1}}(s^{\tau_a})s^{\tau_1}\uno{\tau_1<\infty}).\]

\begin{exer}
Justifique esto cuidadosamente.
Para ello escriba $s^{\tau_a-\tau_1}$ como $f(X_{\tau_1}+1,X_{\tau_1}+2,\dotsc)$ para alguna $f\!:I^\nn\longrightarrow\rr$ adecuada y use \eqref{eq:propMarkF}.
\end{exer}

Lo anterior es igual a
\[\ee_0(\ee_{1}(s^{\tau_a})s^{\tau_1}\uno{\tau_1<\infty})=\ee_{1}(s^{\tau_a})\ee_0(s^{\tau_1}\uno{\tau_1<\infty})=\ee_{0}(s^{\tau_{a-1}})\ee_0(s^{\tau_1})=\phi_{a-1}(s)\phi_1(s),\]
donde usamos que la marcha aleatoria es invariante bajo traslaciones, por lo que el tiempo de pasada a $a$ partiendo desde $1$ tiene la misma distribución que el tiempo de pasada a $a-1$ partiendo desde $0$.
Si repetimos esto inductivamente obtenemos 
\[\phi_a(s)=\phi_1(s)^a.\]

Ahora descomponiendo de acuerdo al primer paso, tenemos
\[\phi_1(s)=p\tts\ee_0(s^{\tau_1}|X_1=1)+q\tts\ee_0(s^{\tau_1}|X_1=-1)=p\tts s+q\tts\ee_{-1}(s^{\tau_1+1}),\]
donde aplicamos la propiedad de Markov (débil); \uexers: justifique este último paso; ¿por qué sale un término $+1$ en el último exponente?
Para la última esperanza usamos de nuevo la invarianza bajo traslación de la marcha, de forma que es igual a $\ee_{0}(s^{\tau_2+1})=~s\phi_2(s)=s\phi_1(s)^2$ (lo que habíamos probado recién) y obtenemos $\phi_1(s)=ps+qs\phi_1(s)^2$, lo que lleva a 
\[\phi_1(s)=\frac{1\pm\sqrt{1-4pqs^2}}{2qs}.\]
Un análisis sencillo muestra que el lado derecho anterior está en $[0,1]$ para todo $s\in[0,1]$ (como debe ser por de $\phi_1(s)$) para la raíz con signo menos, y de acá concluimos que
\[\phi_a(s)=\left(\frac{1-\sqrt{1-4pqs^2}}{2qs}\right)^a.\]
De aquí podemos sacar por ejemplo la distribución de $\tau_a$, mediante
\[\pp_0(\tau_a=k)=\frac{1}{k!}\frac{\d^k}{\d s^k}\phi_a(s)\big|_{s=0}\]
(comparando con el lado derecho de \eqref{eq:phia} y usando unicidad de la expansión en serie de potencias en $s$).

Para el problema que nos interesaba tenemos, usando \eqref{eq:phia} nuevamente,
\[\pp_0(\tau_a<\infty)=\sum_{k\geq0}\pp_0(\tau_a=k)=\lim_{s\to1^-}\phi_a(s)=\left(\frac{1-\sqrt{1-4pq}}{2q}\right).\]
El argumento de la raíz es $(1-2p)^2$, y de esto concluimos que
\[\pp_0(\tau_a<\infty)=
\begin{dcases*}
\left(\frac{p}{q}\right)^a & si $p<\tfrac12$,\\
1 & si $p\geq\tfrac12$.
\end{dcases*}\]
Notar que, como debiera esperarse, cuando la probabilidad de salto a la derecha es mayor que $\frac12$, la marcha alcanza cualquier nivel $a>0$ eventualmente con probabilidad $1$.
Lo que también se concluye, y que es quizás menos claro intuitivamente, es que para el caso $p<\frac12$ siempre hay una probabilidad positiva de nunca alcanzar $a$.

\begin{ex}
Para la cadena de Markov dada por la ruina del jugador, Ej. \ref{ex:ruina}, demuestre que la probabilidad de que el jugador se quede eventualmente sin dinero si parte en $1$ es $(1-p)/p$ si $p>\frac12$ y $1$ si $p\leq\frac12$.
\uindic: si bien puede repetir el análisis de arriba, esto no es necesario y puede comparar directamente con el ejemplo anterior.
¿Puede calcular el tiempo esperado de llegada a $0$ para el caso $p>\frac12$?
\end{ex}

\subsection{Probabilidades de absorción}\label{sec:absor}

Ver P2 aux. 3 \ucmark.

% \subsection{Clasificación de estados}

\subsection{Estructura de clases}

\subsubsection{Clases e irreducibilidad}

\begin{defn}
Sea $X$ una cadena de Markov y sean $i,j\in I$.
Decimos que\MarginNote{Def. ``$i$ lleva a $j$'' ($i\to j$)} \emph{$i$ lleva a $j$}, y escribimos $i\to j$, si $\pp_i(\exists n\geq0\!:X_n=j)>0$.
Decimos que\MarginNote{Def. ``$i$ y $j$ se comunican'' ($i\leftrightarrow j$)} \emph{$i$ y $j$ se comunican}, y escribimos $i\leftrightarrow j$, si $i\to j$ y $j\to i$.
\end{defn}

\begin{ex}
Consideremos una cadena de Markov a valores en $I=\{1,2,3,4,5\}$.
En el siguiente grafo dirigido representamos por aristas entre nodos las transiciones que suceden con probabilidad estrictamente positiva (es decir, ponemos una arista de $i$ a $j$ si y solo si $p_{ij}>0$); usaremos esta convención varias veces a lo largo de las notas.
\begin{center}
\begin{tikzpicture}[-latex,auto,node distance=1.2cm and 2.5cm,on grid,semithick,state/.style ={circle,top color=white,draw}]
\node[state] (A) {\footnotesize$1$};
\node[state] (B) [right =of A] {\footnotesize$2$};
\node[state] (C) [right =of B] {\footnotesize$3$};
\node[state] (D) [below =of A] {\footnotesize$4$};
\node[state] (E) [right =of D] {\footnotesize$5$};
\path (A) edge [bend left =15] (B);
\path (B) edge [bend left =15] (C);
\path (C) edge [bend left =15] (B);
\path (A) edge [bend right =15] (D);
\path (D) edge [bend left =15] (E);
% \path (A) edge [bend right = -15] node[below =0.15 cm] {$1/2$} (C);
% \path (A) edge [bend left =25] node[above] {$1/4$} (B);
% \path (B) edge [bend left =15] node[below =0.15 cm] {$1/2$} (A);
% \path (C) edge [bend left =15] node[below =0.15 cm] {$1/2$} (B);
% \path (B) edge [bend right = -25] node[below =0.15 cm] {$1/2$} (C);
\end{tikzpicture}
\end{center}
Para esta cadena tenemos $1\to2$, $2\leftrightarrow3$ y $1\to5$, pero $4\nrightarrow2$.
\end{ex}

\begin{prop}\MarginNote{Caract. de $i \to j $}
Las siguientes son equivalentes:
\begin{enumerate}[label=\uptext{(\arabic*)}]
\item $i\to j$.
\item Existen $n\in\nn$ e $i_1,\dotsc,i_n$ tales que $p_{i_1i_2}p_{i_2i_3}\dotsm p_{i_{n-1}i_n}>0$.
\item Existe $n\geq0$ tal que $\pp_i(X_n=j)>0$.
\end{enumerate}
\end{prop}

\begin{proof}

Que (2) implica (1) es directo.
Para ver que (1) implica (3) probamos la contrarecíproca: asumiendo que $\pp_i(X_n=j)=0$ para todo $n$, tenemos que 
\[\pp_i(\exists n\geq0\!:X_n=j)\leq\sum_{n\geq0}\pp_i(X_n=j)=0.\]
Veamos ahora que (3) implica (2): tenemos que
\begin{multline*}
0<\pp_i(X_n=j)=\!\!\sum_{i_1,\dotsc,i_{n-1}}\pp_i(X_1=i_1,\dotsc,X_{n-1}=i_{n-1},X_n=j)\\
=\!\!\sum_{i_1,\dotsc,i_{n-1}}p_{ii_1}p_{i_1i_2}\dotsm p_{i_{n-1}j},
\end{multline*}
y luego alguno de los sumandos debe ser positivo.
\end{proof}

\begin{prop}\MarginNote{$\to$ es refl. y trans. y $\leftrightarrow$ es de equiv.}
La relación $\to$ es reflexiva y transitiva y la relación $\leftrightarrow$ es de equivalencia.
\end{prop}

\begin{proof}
Demostremos que $\to$ es transitiva, el resto queda de ejercicio.
Supongamos que $i\to j$ y $j\to k$, de manera que por la proposición anterior existen $m,n\in\nn$ tales que $p^{(m)}_{ij}>0$ y $p^{(n)}_{jk}>0$.
Por Chapman-Kolmogorov tenemos $p^{(m+n)}_{ik}=\sum_{\ell\in\nn}p^{(m)}_{i\ell}p^{(n)}_{\ell k}\geq p^{(m)}_{ij}p^{(n)}_{jk}>0$, y entonces $i\to k$.
\end{proof}

\begin{defn}\tbf{Clase y clase cerrada}\MarginNote{Clase y clase cerrada}
La \emph{clase} de $i\in I$, que denotaremos por $C(i)$, es la clase de equivalencia de $i$ en $I$ con respecto a la relación $\leftrightarrow$.
Decimos que una clase $C$ es \emph{cerrada} si cada vez que $i\in C$ y $i\to j$ se tiene necesariamente que $j\in C$.
\end{defn}

\begin{ex}
\leavevmode
\begin{center}
\begin{tikzpicture}[-latex,auto,node distance=0.8cm and 3cm,on grid,semithick,state/.style ={circle,top color=white,draw}]
\node[state] (C) {\footnotesize$3$};
\node[state] (A) [above left =of C] {\footnotesize$1$};
\node[state] (B) [below left =of C] {\footnotesize$2$};
\node[state] (D) [above right =of C] {\footnotesize$4$};
\node[state] (E) [below right =of C] {\footnotesize$5$};
\node[state] (F) [right =of E] {\footnotesize$6$};
\path (A) edge [bend right =15] (B);
\path (B) edge [bend right =15] (A);
\path (B) edge [bend left =15] (C);
\path (C) edge [bend left =15] (D);
\path (C) edge [bend right =15] (E);
\path (E) edge [bend left =15] (F);
\path (F) edge [bend left =15] (E);
% \path (A) edge [bend right = -15] node[below =0.15 cm] {$1/2$} (C);
% \path (A) edge [bend left =25] node[above] {$1/4$} (B);
% \path (B) edge [bend left =15] node[below =0.15 cm] {$1/2$} (A);
% \path (C) edge [bend left =15] node[below =0.15 cm] {$1/2$} (B);
% \path (B) edge [bend right = -25] node[below =0.15 cm] {$1/2$} (C);
\end{tikzpicture}
\end{center}
Para esta cadena hay tres clases: $\{1,2,3\}$, $\{4\}$, $\{5,6\}$.
Las últimas dos son cerradas.
\end{ex}

\begin{defn}\tbf{Irreducibilidad}\MarginNote{Irreducibilidad}
Decimos que una cadena de Markov $X$ es \emph{irreducible}, o alternativamente que su matriz de transición $P$ lo es, si $X$ tiene una única clase.
\end{defn}

Notar que si $X$ es irreducible, su única clase es necesariamente cerrada.

\begin{ex}\label{ex:irred}
\leavevmode
\begin{center}
\begin{tikzpicture}[-latex,auto,node distance=1cm and 2.5cm,on grid,semithick,state/.style={circle,draw,fill=black,scale=0.5}]
\node[state] (A) {};
\node[state] (B) [right=of A] {};
\node[state] (C) [below=of A] {};
\node[state] (D) [below=of B] {};
\path (A) edge [bend left=15] (B);
\path (B) edge [bend left=15] (D);
\path (D) edge [bend left=15] (B);
\path (D) edge [bend left=15] (C);
\path (C) edge [bend left=15] (A);
\path (D) edge [bend left=5] (A);
\end{tikzpicture}
\end{center}
Esta cadena es irreducible.
La marcha aleatoria simple en $\zz$ también lo es.
Pero
\begin{center}
\begin{tikzpicture}[-latex,auto,node distance=2cm and 1.2cm,on grid,semithick,state/.style={circle,draw,fill=black,scale=0.5}]
\node[state] (A) {};
\node[state] (A-) [left=of A] {};
\node[state] (B) [right=of A] {};
\node[state] (C) [right=of B] {};
\node[state] (D) [right=of C] {};
\node[state] (E) [right=of D] {};
\node[state] (F) [right=of E] {};
\path (A) [bend left=15] edge (A-);
\path (A-) [bend left=15] edge (A);
\path (B) edge (A);
\path (B) [bend left=15] edge (C);
\path (C) [bend left=15] edge (B);
\path (C) [bend left=15] edge (D);
\path (D) [bend left=15] edge (C);
\path (D) [bend left=15] edge (E);
\path (E) [bend left=15] edge (D);
\path (E) [bend left=15] edge (F);
\path (F) [bend left=15] edge (E);
\end{tikzpicture}
\end{center}
no es irreducible, pues los dos estados de más a la izquierda forman una clase separada del resto de los estados de la cadena.
\end{ex}

La noción de irreducibilidad será clave en nuestro estudio de cadenas de Markov: típicamente asumiremos que nuestras cadenas son irreducibles.
La idea es que cuando hay más de una clase, la cadena puede estudiarse por separado en cada una de ellas.

\subsubsection{Periodo}

Otra noción importante, que asumiremos a menudo de nuestras cadenas, es la de aperiodicidad.
Partimos por la siguiente definición.

\begin{defn}\tbf{Periodo}\MarginNote{Periodo}
El \emph{periodo} de $i\in I$ es
\[\d(i)=\uptext{mcd}\{n>0\!:p^{(n)}_{ii}>0\},\]
donde mcd denota al máximo común divisor (y tomamos $\uptext{mcd}(\emptyset)=0$).
\end{defn}

\begin{ex}\label{ex:periodo}
La marcha aleatoria en $\zz$ tiene período 2 (\uexers: verifíquelo); esto implica en este caso, por ejemplo, que si la marcha comienza en un estado par, entonces estará en un estado par en cada instante de tiempo par y en uno impar en cada instante de tiempo impar.
La primera cadena del Ej. \ref{ex:irred} tiene periodo $1$.
\end{ex}

\begin{defn}\MarginNote{Propiedad de Clase}
Decimos que una propiedad definida para estados de una cadena de Markov es \emph{propiedad de clase} si se tiene que cada vez que $i$ satisface la propiedad y $j\in C(i)$ entonces $j$ satisface la propiedad.
\end{defn}

\begin{prop}\label{prop:periodo-clase}
El periodo es propiedad de clase (es decir, $i\leftrightarrow j$ implica que $\d(i)=\d(j)$).
\end{prop}

\begin{proof}
Tomemos $i\neq j$ en la misma clase, de forma que existen $m$ y $n$ tales que $p^{(m)}_{ij}>0$ y $p^{(n)}_{ji}>0$.
Además sabemos que existe $r>0$ tal que $p^{(r)}_{ii}>0$ (por ejemplo podemos tomar $r=m+n$ por Chapman-Kolmogorov).
Luego para todo $k\geq0$ Chapman-Kolmogorov da
\[p^{(n+m+kr)}_{jj}\geq p^{(m)}_{ji}(p^{(r)}_{ii})^kp^{(n)}_{ij}\]
(\uexers: verifique esto), y concluimos que $\d(j)$ es divisor de $m+n+kr$ para todo $k\geq0$.
En particular, $\d(j)$ es divisor de $(m+n+2r)-(m+n+r)=r$.
Obtenemos como conclusión que para cada $r>0$ tal que $p^{(r)}_{ii}>0$ se tiene que $\d(j)$ divide a $r$ y luego, en particular, $\d(j)$ divide a $\d(i)$.
Intercambiando los roles de $i$ y $j$ concluimos que $\d(i)$ divide a $\d(j)$, y luego $\d(i)=\d(j)$.
\end{proof}

\begin{defn}\tbf{Aperiodicidad}\MarginNote{Aperiodicidad}
Decimos que un estado $i\in I$ es \emph{aperiódico} si $\d(i)=1$.
Decimos que una cadena de Markov es \emph{aperiódica} si todos sus estados lo son.
\end{defn}

Ciertos aspectos del estudio de cadenas de Markov se simplifican al considerar cadenas aperiódicas, que evitan comportamientos como los descritos en el Ex. \ref{ex:periodo} para la marcha aleatoria simple en $\zz$.
Sin embargo, y tal como el mismo ejemplo muestra, muchas cadenas naturales no son aperiódicas, por lo que estas sí tendrán que ser consideradas en nuestro estudio.

El siguiente resultado se demuestra de manera similar a la Prop. \ref{prop:periodo-clase}:

\begin{lem}
Sea $X$ irreducible y sea $i\in I$.
Si $i$ es aperiódico entonces $X$ lo es.
Más aún, en este caso se tiene que para todo $i,j\in I$ existe un $n_0\geq0$ tal que $p_{ij}^{(n)}>0$ para todo $n\geq n_0$.
\end{lem}

\begin{exer}
Demuestre el lema anterior.
\end{exer}

\subsection{Recurrencia y transiencia}

Ahora examinaremos uno de los aspectos claves en el estudio de cadenas de Markov: la distinción entre recurrencia y transiencia.

\subsubsection{Definición y propiedades básicas}

\begin{defn}\tbf{Tiempo de retorno}\MarginNote{Tiempo de retorno}
Dada una cadena de Markov $X$, definimos su \emph{tiempo de retorno} (o \emph{tiempo de llegada}) a $i\in I$ como
\[\tau_i=\inf\{n\geq1\!:X_n=i\}.\]
Más generalmente, si $A\subseteq I$, escribimos
\[\tau_i=\inf\{n\geq1\!:X_n\in A\}.\]
\end{defn}

Los anteriores son claramente tiempos de parada (\uexers: demuéstrelo).
Para efectos del estudio de recurrencia de cadenas de Markov que haremos a continuación, típicamente estaremos interesados en el caso en que la cadena comienza en $i$ y nos preguntamos por el primer tiempo en que vuelve a $i$, lo que explica llamar a $\tau_i$ el tiempo de retorno a $i$ (pero, por supuesto, la definición tiene sentido para la cadena comenzando en cualquier estado $j$).

\begin{defn}
El \emph{número de visitas} a $i$ es
\[N_i=\sum_{n\geq0}\uno{X_n=i}.\]
\end{defn}

\begin{defn}\tbf{Recurrencia y transiencia}
Decimos que $i\in I$ es\MarginNote{Recurrencia} \emph{recurrente} si
\[\pp_i(\tau_i<\infty)=1.\]
En caso contrario, decimos que $i$ es \emph{transiente}.\MarginNote{Transiencia}
\end{defn}

En palabras, un estado es recurrente si partiendo de $i$ la cadena vuelve a $i$ en el futuro con probabilidad $1$.
Nuestro objetivo es demostrar la siguiente dicotomía entre recurrencia y transiencia.
Enunciaremos el resultado por separado en los casos recurrente y transiente, aunque son esencialmente equivalentes.

\begin{thm}\label{thm:rec-trans}\MarginNote{Teo: Dicotomía entre recurrencia y transiencia}
Sea $i\in I$.
\begin{enumerate}[label=\uptext{\arabic*.}]
\item \uptext{($i$ recurrente)}\enspace Las siguientes son equivalentes:
\begin{enumerate}[label=\uptext{(\roman*)}]
\item $\pp_i(\tau_i<\infty)=1$.
\item $\pp_i(N_i=\infty)=1$.
\item $\ee_i(N_i)=\infty$.
\end{enumerate}
\item \uptext{($i$ transiente)}\enspace Las siguientes son equivalentes:
\begin{enumerate}[label=\uptext{(\roman*)}]
\item $\pp_i(\tau_i<\infty)<1$.
\item $\pp_i(N_i=\infty)=0$.
\item $\ee_i(N_i)<\infty$.
\end{enumerate}
\end{enumerate}
\end{thm}

% \begin{thm}\label{thm:rec-trans}
% \leavevmode
% \begin{enumerate}[label=\uptext{\arabic*.}]
% \item \uptext{(Caso recurrente)}\enspace Las siguientes son equivalentes:
% \begin{enumerate}[label=\uptext{(\roman*)}]
% \item $\pp_i(\tau_i<\infty)=1$.
% \item $\pp_i(N_i=\infty)=1$.
% \item $\ee_i(N_i)=\infty$.
% \end{enumerate}
% \item \uptext{(Caso transiente)}\enspace Las siguientes son equivalentes:
% \begin{enumerate}[label=\uptext{(\roman*)}]
% \item $\pp_i(\tau_i<\infty)<1$.
% \item $\pp_i(N_i=\infty)=0$.
% \item $\ee_i(N_i)<\infty$.
% \end{enumerate}
% \end{enumerate}
% \end{thm}

El contenido esencial de este teorema es el hecho de que la recurrencia de un estado $i\in I$ equivale a que, partiendo de $i$, la cadena visita $i$ una infinidad de veces.

Partimos con un lema sencillo:

\begin{lem}\MarginNote{$\ee_i(N_i)=\sum_{n\geq0}p_{ii}^{(n)}$.}
$\ee_i(N_i)=\sum_{n\geq0}p_{ii}^{(n)}$.
\end{lem}

\begin{proof}
\[\textstyle\ee_i(N_i)=\ee_i(\sum_{n\geq0}\uno{X_n=i})=\sum_{n\geq0}\ee_i(\uno{X_n=i})=\sum_{n\geq0}\pp_i(X_n=i)=\sum_{n\geq0}p_{ii}^{(n)},\]
donde en la segunda igualdad usamos el Teorema de Convergencia Monótona.
\end{proof}

Como consecuencia de este lema y el teorema obtenemos un criterio explícito para chequear si un estado es recurrente o transiente:

\begin{cor}\MarginNote{Caract. para det. si un est. es recurrente}
Un estado $i\in I$ es recurrente si y solo si $\sum_{n\geq0}p_{ii}^{(n)}=\infty$.
\end{cor}

\begin{ex}\label{ex:srw-rec-1}
Veremos más adelante (\ucmark) que para la marcha aleatoria simple en $\zz$ se tiene $p^{(2n)}_{00}\sim\frac{(4p(1-p))^n}{\sqrt{\pi n}}$ (mientras que $p^{(2n)}_{00}=0$ para todo $n$).
En el caso simétrico, $p=\frac12$, tenemos $4p(1-p)=1$, y por lo tanto $\sum_{n\geq0}p_{ii}^{(n)}=\infty$, de donde concluimos que la marcha aleatoria es recurrente.
En el caso asimétrico, $p\neq\frac12$, como la función $p\longmapsto p(1-p)$ alcanza su máximo en $[0,1]$ en $p=\frac12$, tenemos $4p(1-p)<1$ y por lo tanto $\sum_{n\geq0}p_{ii}^{(n)}<\infty$, de donde se sigue que la marcha aleatoria es transiente.
\uexer: ¿Es este resultado consistente con lo que obtuvimos en el ejemplo de la Sección \ref{sec:pasada}?
\end{ex}

Para la demostración del Teorema \ref{thm:rec-trans} utilizaremos una idea que se conoce a veces como\MarginNote{Descomposición por tiempo de llegada} descomposición por tiempo de llegada.
Esto lo hacemos en el resultado siguiente.
Sea
\begin{equation}
u^{(n)}_{ij}=\pp_i(\tau_j=n).\label{eq:uijn}
\end{equation}
Notemos que $u^{(0)}_{ii}=0$, por definición de $\tau_i$.

\begin{lem}\MarginNote{Fórm. para $p_{ij}^{(n)}$}
Para todo $n\geq0$ se tiene que
\[p_{ij}^{(n)}=\sum_{k=1}^nu_{ij}^{(k)}p_{jj}^{n-k}\uno{n>0}+\uno{n=0,i=j}.\]
\end{lem}

\begin{proof}
El caso $n=0$ es trivial, así que asumamos que $n>0$.
Tenemos
\NAM[
\[p_{ij}^{(n)}=\pp_i(X_n=j)=\ee_i(\uno{X_n=j}\uno{\tau_j\leq n})=\ee_i(\ee(\uno{X_n=j}|\cf_{\tau_j})\uno{\tau_j\leq n})=\sum_{k=1}^n\ee_i(\ee(\uno{X_n=j}|\cf_{\tau_j})\uno{\tau_j=k}),\]%=\sum_{k=1}^n\ee_i(\ee(\uno{X_n=j}|\cf_{\tau_j})\uno{\tau_j=k})
]{
\begin{multline*}
    p_{ij}^{(n)}=\pp_i(X_n=j)=\ee_i(\uno{X_n=j}\uno{\tau_j\leq n})=\ee_i(\ee(\uno{X_n=j}|\cf_{\tau_j})\uno{\tau_j\leq n})\\ =\sum_{k=1}^n\ee_i(\ee(\uno{X_n=j}|\cf_{\tau_j})\uno{\tau_j=k}),
\end{multline*}
}

donde usamos el hecho que $X_n=j$ implica que $\tau_j\leq n$.
Ahora usamos la propiedad de Markov fuerte en la esperanza condicional.
Para ello reescribimos el término $k$-ésimo de la suma al lado derecho como $\ee_i(\ee(\uno{X_n-k+\tau_j=j}|\cf_{\tau_j})\uno{\tau_j=k})$ (\uexers: ¿por qué podemos hacer esto?), de manera que la esperanza condicional que aparece adentro es igual a $\ee_{X_{\tau_j}}\!(\uno{X_n-k=j})$.
Luego
\[p_{ij}^{(n)}=\sum_{k=1}^n\ee_i(\pp_j(X_{n-k}=j)\uno{\tau_j=k})=\sum_{k=1}^np^{(n-k)}_{jj}\pp_i(\tau_j=k)=\sum_{k=1}^np^{(n-k)}_{jj}u_{ij}^{(k)},\]
que es lo queríamos probar.
\end{proof}

Estamos listos para demostrar la dicotomía entre recurrencia y transiencia.
Antes de hacerlo introducimos una notación que usaremos en la demostración.

\begin{defn}\label{def:k-retorno}\MarginNote{Tiempo del $k$-ésimo retorno}
Definimos en \emph{tiempo del $k$-ésimo retorno} a $i$, $\tau^{(k)}_i$, inductivamente como
\[\tau^{(1)}_i=\tau_i,\qquad\tau^{(k)}_i=\inf\{n>\tau^{(k-1)}_i\!:X_n=i\}\quad\text{para $k>1$}.\]
\end{defn}

\begin{proof}[Demostración del Teorema \ref{thm:rec-trans}]
Comenzamos por el caso recurrente, probando la equivalencia entre (i) y (iii).
Sean
\begin{equation}
p_{ii}(z)=\sum_{n\geq0}p^{(n)}_{ii}z^n,\qquad u_{ii}(z)=\sum_{n\geq0}u^{(n)}_{ii}z^n.
\end{equation}
Como $p^{(n)}_{ii}$ y $u^{(n)}_{ii}$ están en $[0,1]$ para todo $n$, ambas series son absolutamente convergentes si $|z|<1$.
Para tales $z$ tenemos entonces
% \begin{align}
\[u_{ii}(z)p_{ii}(z)=\sum_{n\geq0}\sum_{k=0}^nu_{ii}^{(k)}z^kp_{ii}^{(n-k)}z^{n-k}=\sum_{n\geq0}z^n\sum_{k=0}^nu_{ii}^{(k)}p_{ii}^{(n-k)}\]
% \end{align}
(simplemente por regla de multiplicación de series de potencias).
Sea $S_n=\sum_{k=0}^nu_{ii}^{(k)}p_{ii}^{(n-k)}$, que es el coeficiente $n$-ésimo de la serie de potencias del lado derecho arriba.
Si $n=0$ tenemos $S_n=u_{ii}^{(0)}p_{ii}^{(0)}=0$.
En caso contrario, gracias al lema anterior tenemos $S_n=u_{ii}^{(0)}p_{ii}^{(0)}+p_{ii}^{(n)}$.
Concluimos entonces que para todo $n\geq0$ se tiene $S_n=p_{ii}^{(n)}\uno{n>0}=p_{ii}^{(n)}-\uno{n=0}$ (\uexers: ¿por qué se tiene esto último?).
Usando esto arriba obtenemos
\[u_{ii}(z)p_{ii}(z)=\sum_{n\geq0}(p_{ii}^{(n)}-\uno{n=0})z^n=\sum_{n\geq0}p_{ii}^{(n)}-1=p_{ii}(z)-1\]
o, equivalentemente,
\[p_{ii}(z)(1-u_{ii}(z))=1\]
para todo $|z|<1$.
Ahora notamos que
\begin{align}
\lim_{z\to1^-}p_{ii}(z)&=\sum_{n\geq0}p_{ii}^{(n)}=\ee_i(N_i),\\
\lim_{z\to1^-}u_{ii}(z)&=\sum_{n\geq0}u_{ii}^{(n)}=\sum_{n\geq0}\pp_i(\tau_i=n)=\pp_i(\tau_i<\infty).
\end{align}
Pero, por lo anterior,
\[\lim_{z\to1^-}p_{ii}(z)(1-u_{ii}(z))=1.\]
Luego $\ee_i(N_i)=\infty$ implica que necesariamente $\pp_i(\tau_i<\infty)=1$ y viceversa, lo que nos dice que (i) y (iii) son equivalentes.

Que (ii) implica (iii) es directo.
Veamos entonces que (i) implica (ii).
Tenemos que
\[\{N_i=\infty\}=\{\tau_i^{(k)}~\forall\,k\geq1\}.\]
\uexer: Pruebe lo anterior.
Luego
\[\pp_i(N_i=\infty)=\pp_i\Big({\textstyle\bigcap_{k\geq1}}\{\tau^{(k)}_i<\infty\}\Big)=\lim_{k\to\infty}\pp_i(\tau^{(k)}_i<\infty),\]
donde en la segunda igualdad usamos que los eventos $\{\tau^{(k)}_i<\infty\}$ son decrecientes.
De esta forma la implicancia que queremos probar equivale a probar que 
\begin{equation}
\pp_i(\tau_i<\infty)=1\quad\Longrightarrow\quad\lim_{k\to\infty}\pp_i(\tau^{(k)}_i<\infty)=1.
\end{equation}
Y el lado derecho equivale a pedir que $\pp_i(\tau^{(k)}_i<\infty)=1$ para todo $k\geq1$ (\uexers: ¿por qué?).
Probaremos esto por inducción.
El caso $k=1$ es la hipótesis.
Para el paso inductivo usamos el siguiente:
\begin{exer}
Demuestre que para todo $k\geq1$,
\[\pp_i(\tau^{(k+1)}_i<\infty)=\pp_i(\tau_i<\infty)\pp_i(\tau^{(k)}_i<\infty).\]
\end{exer}
\noindent La conclusión ahora es directa.

Resta probar el caso transiente.
Del caso recurrente obtenemos directamente que
\[\pp_i(\tau_i<\infty)<1\quad\Longleftrightarrow\quad\pp_i(N_i=\infty)<1\quad\Longleftrightarrow\quad\ee_i(N_i)<\infty.\]
Luego lo que falta demostrar es que las propiedades anteriores equivalen a $\pp_i(N_i=\infty)=0$, pero esto es directo: esta propiedad implica trivialmente que $\pp_i(N_i=\infty)<1$, mientras que la propiedad $\ee_i(N_i)<\infty$ implica que $\pp_i(N_i=\infty)=0$.
\end{proof}

La siguiente es una consecuencia más o menos directa del argumento usado en la demostración para probar que (ii) implica (iii):

\begin{prop}\label{prop:geomNi}
Sea $\theta_i=\pp_i(\tau_i<\infty)$.
Entonces para todo $k\geq1$,
\[\pp_i(\tau^{(k)}_i<\infty)=\theta_i^k.\]
En particular, en el caso $i$ transiente ($\theta_i<1$) tenemos
\[\pp_i(N_i=k)=\theta_i^k(1-\theta_i)\qquad\text{y}\qquad\ee_i(N_i)=\frac{\theta_i}{1-\theta_i},\]
es decir, $N_i$ es una variable Geom$[\theta_i]$.
\end{prop}

\begin{exer}
Demuestre la Proposición \ref{prop:geomNi} y asegúrese de comprender su contenido (¿por qué es natural esperar que $N_i$ sea una variable geométrica?).
\end{exer}

\subsubsection{Más propiedades}

En el resto de esta sección profundizaremos nuestro estudio de recurrencia y transiencia.

\begin{prop}\MarginNote{Recurrencia es prop. de clase}
La recurrencia es propiedad de clase, i.e. si $i\leftrightarrow j$ entonces $i$ es recurrente si y solo si $j$ es recurrente.
\end{prop}

\begin{proof}
Supongamos que $i\leftrightarrow j$, de manera que existen $m,n\geq0$ tal que $p^{(m)}_{ij}>0$ y $p^{(n)}_{ji}>0$, y asumamos que $i$ es transiente.
Por Chapman-Kolmogorov tenemos $p^{(m+n+r)}_{ii}\geq p^{(m)}_{ij}p^{(r)}_{jj}p^{(n)}_{ji}$ para cada $r\geq0$, y luego
\[\ee_j(N_j)=\sum_{r\geq0}p^{(r)}_{jj}\leq\frac{1}{p^{(m)}_{ij}p^{(n)}_{ji}}\sum_{r\geq0}p^{(m+n+r)}_{ii}\leq\frac{1}{p^{(m)}_{ij}p^{(n)}_{ji}}\ee_i(N_i)<\infty,\]
pues $i$ es transiente.
Deducimos entonces que $j$ es transiente.
La otra implicancia sale igual, intercambiando $i$ y $j$.
\end{proof}

En vista del resultado anterior hacemos la siguiente:

\begin{defn}
Decimos que una clase $C$ es \emph{recurrente} si algún (o equivalentemente todo) $i\in C$ lo es.
Decimos que la cadena $X$ es \emph{recurrente}, o que su matriz de transición $P$ lo es, si todo $i\in I$ es recurrente.
\end{defn}

\begin{prop}\label{prop:recprops}
\leavevmode
\begin{enumerate}[label=\uptext{(\roman*)}]
\item Para $i\neq j$ se tiene $i\rightarrow j$ si y solo si $\pp_i(\tau_j<\infty)>0$.
\item $\pp_i(N_j=\infty)=\pp_i(\tau_j<\infty)\pp_j(N_j=\infty)$.
\item Si $i\rightarrow j$ e $i$ es recurrente entonces $\pp_i(\tau_j<\infty)=1$.
\end{enumerate}
\end{prop}

\begin{proof}
La parte (i) es trivial.

\begin{exer}
Use la propiedad de Markov para demostrar (ii).
\end{exer}
\noindent La parte (iii) es la más interesante.
Partimos con la descomposición
\[\pp_i(\tau_j<\infty)=\pp_i(\tau_j<\tau_i)+\pp_i(\tau_i\leq\tau_j,\,\tau_j<\infty).\]
Para el segundo término del lado derecho usamos la propiedad de Markov fuerte a tiempo $\tau_i$:
\begin{align}
\pp_i(\tau_i\leq\tau_j,\,\tau_j<\infty)&=\ee_i(\ee(\uno{\tau_j<\infty}|\cf_{\tau_i})\uno{\tau_i\leq\tau_j})=\ee_i(\ee_{X_{\tau_i}}(\uno{\tau_j<\infty})\uno{\tau_i\leq\tau_j})\\
&=\ee_i(\pp_i(\tau_j<\infty)\uno{\tau_i\leq\tau_j})=\pp_i(\tau_j<\infty)\pp_i(\tau_i\leq\tau_j)
\end{align}
donde, en la primera igualdad, usamos que $\{\tau_i\leq\tau_j\}\in\cf_{\tau_i}$.
Deducimos que $\pp_i(\tau_j<\infty)=\pp_i(\tau_j<\tau_i)+\pp_i(\tau_j<\infty)\pp_i(\tau_i\leq\tau_j)$, que podemos reescribir como
\[\pp_i(\tau_j<\infty)\pp_i(\tau_j<\tau_i)=\pp_i(\tau_j<\tau_i).\]
Luego basta probar que $\pp_i(\tau_j<\tau_i)>0$, que es consecuencia de la hipótesis $i\rightarrow j$ y se deja como ejercicio.
\end{proof}

\begin{exer}
Termine la demostración del resultado anterior.
\end{exer}

\begin{cor}
Sea $X$ una cadena de Markov irreducible.
Entonces $X$ visita todo $i\in I$ una infinidad de veces con probabilidad $1$ (es decir, $\pp(N_i=\infty~\forall\,i\in I)=1$).
\end{cor}

\begin{exer}
Demuestre lo anterior.
\end{exer}

\NAM[]{\newpage}

\begin{prop}\MarginNote{Toda clase Recurrente es cerrada}
Toda clase recurrente es cerrada.
\end{prop}

\begin{proof}
Sea $C$ una clase recurrente y sean $i,j$ tales que $i\in C$ e $i\rightarrow j$.
Debemos demostrar entonces que $j\in C$, es decir que $j\rightarrow i$.
Supongamos por contradicción que esto no se tiene, es decir, que $j\nrightarrow i$.
Sabemos que hay un $n\geq0$ tal que $p_{ij}^{(n)}>0$.
Tenemos que $\pp_i(X_n=j,\,N_i<\infty)=\pp_i(X_n=j)$, pues $j\nrightarrow i$.
Luego
\[\pp_i(N_i<\infty)\geq\pp_i(X_n=j,N_i<\infty)=\pp_i(X_n=j)=p_{ij}^{(n)}>0,\]
de donde se sigue que $i$ es transiente, lo que nos da la contradicción buscada.
\end{proof}


\begin{prop}
Si $j$ es transiente entonces $\ee_i(N_j)=\sum_{n\geq0}p_{ij}^{(n)}<\infty$ para todo $i\in I$.
\end{prop}

\begin{proof}
Esto lo sabemos ya en el caso $i=j$.
Para el caso $i\neq j$ usaremos la misma idea que en la demostración de ese caso (Teorema \ref{thm:rec-trans}).
\begin{exer}
Sean $p_{ij}(z)=\sum_{n\geq0}p^{(n)}_{ij}z^n$ y $u_{ij}(z)=\sum_{n\geq0}u^{(n)}_{ij}z^n$, con $u^{(n)}_{ij}$ como en \eqref{eq:uijn}.
Demuestre que para todo $|z|<1$ se tiene
\[p_{ij}(z)=u_{ij}(z)p_{jj}(z).\]
\end{exer}
\noindent Usando el ejercicio y tomando $z\to1^-$ obtenemos (usando el Teorema de Convergencia Monótona)
\[\sum_{n\geq0}p_{ij}^{(n)}=\left(\sum_{n\geq0}u_{ij}^{(n)}\right)\left(\sum_{n\geq0}p_{jj}^{(n)}\right)=\pp_i(\tau_j<\infty)\ee_j(N_j),\]
y luego el lado derecho es finito pues $j$ es transiente.
\end{proof}

\begin{cor}\label{cor:finito-rec}
Si $I$ es finito entonces existe un estado recurrente.
En particular, si $I$ es finito y $X$ es irreducible entonces $X$ es recurrente.
\end{cor}

\begin{proof}
Como $I$ es finito tenemos
\[\textstyle\sum_{j\in I}\lim_{n\to\infty}p^{(n)}_{ij}=\lim_{n\to\infty}\sum_{j\in I}p^{(n)}_{ij}=1.\]
Pero si todo $j$ fuera transiente, la suma del lado izquierdo necesariamente sería nula (\uexers: ¿por qué?).
Deducimos entonces que debe existir algún $j$ recurrente.
\end{proof}

\subsubsection{Caracterización}

Concluimos esta sección con una caracterización algebraica de recurrencia en base a propiedades de la matriz de transición $P$.

\begin{defn}\tbf{Funciones armónicas}
Sea $i\in I$.
Decimos que una función $f\!:I\longrightarrow\rr$ es \emph{armónica} en $i$ (con respecto a $P$) si $Pf(i)=\sum_{j\in I}p_{ij}f(j)=f(i)$.
Decimos que $f$ es \emph{superarmónica} (o \emph{subarmónica}) en $i$ si $Pf(i)\leq f(i)$ (o $Pf(i)\geq f(i)$).
\end{defn}

\begin{thm}\tbf{Caracterización de recurrencia}
Sea $X\sim\CM(P)$ irreducible.
Entonces $X$ es recurrente si y solo si existe un $i_0\in I$ (equivalentemente, para todo $i_0\in I$) que satisface lo siguiente: si $f$ es armónica en $I\setminus\{i_0\}$ (i.e. $\sum_{j}p_{ij}f(j)=f(i)$ para todo $i\neq i_0$) y acotada, entonces $f$ es constante.
\end{thm}

\begin{proof}
Supongamos primero que la propiedad se cumple.
Sea $\tilde\tau_{i_0}=\inf\{k\geq0\!:X_k=i_0\}$ (este es el primer tiempo de llegada a $i_0$, notar que permitimos que este tiempo sea $0$).
Definimos $f\!:I\longrightarrow\rr$ mediante $f(i_0)=1$ y $f(i)=\pp_i(\tilde\tau_{i_0}<\infty)$ para $i\neq i_0$.
Para $i\neq i_0$ tenemos
\begin{align}
f(i)&=\sum_{j\in I}\pp_{i}(\tilde\tau_{i_0}<\infty,\,X_1=j)=p_{ii_0}+\sum_{j\neq i_0}p_{ij}\pp_{j}(\tilde\tau_{i_0}<\infty)=p_{ii_0}+\sum_{j\neq i_0}p_{ij}f(j).
\end{align}
\uexer: Justifique este cálculo.
Como $f(i_0)=1$, lo anterior equivale a afirmar que $f(i)=Pf(i)$ para todo $i\neq i_0$ (esto es el resultado de la Sección \ref{sec:absor} \ucmark).
Luego $f$ es armónica en $I\setminus\{i_0\}$, y es claramente acotada, de forma que debe ser constante.
Pero entonces para todo $i\neq i_0$ tenemos 
\[\pp_i(\tau_{i_0}<\infty)=\pp_i(\tilde\tau_{i_0}<\infty)=f(i)=p_{ii_0}+\sum_{j\neq i_0}p_{ij}=1.\]
Argumentando como en la demostración de Prop. \ref{prop:recprops}(iii) tenemos, para $i\neq i_0$,
\[\pp_{i_0}(\tau_{i_0}<\infty)=\pp_{i_0}(\tau_{i_0}<\tau_i)+\pp_{i_0}(\tau_i\leq\tau_{i_0})\pp_i(\tau_{i_0}<\infty).\]
Acabamos de probar que el factor de más a la derecha es igual a $1$, de donde obtenemos $\pp_{i_0}(\tau_{i_0}<\infty)=1$.
Luego $i_0$ es recurrente, y por irreducibilidad la cadena completa lo es.

Ahora asumamos que $X$ es recurrente y, para $i_0\in I$ dado, tomemos $f$ acotada y armónica en $I\setminus\{i_0\}$.
Definamos una matriz de transición auxiliar $\hat P$ dada por $\hat p_{ij}=p_{ij}$ para $i\neq i_0$ y $\hat p_{i_0j}=\uno{j=i_0}$.
Afirmamos que entonces $f$ es armónica con respecto a $\hat P$ en todo $I$.
En efecto, $\hat Pf(i)=Pf(i)=f(i)$ para $i\neq i_0$, mientras que trivialmente $\hat Pf(i_0)=f(i_0)$.
En otras palabras, tenemos $\hat Pf=f$, e iterando obtenemos $\hat P^{(n)}f=f$ para todo $n\geq0$.
Luego escribiendo $f(i)=\hat p^{(n)}_{ii_0}f(i_0)+\sum_{j\neq i_0}\hat p^{(n)}_{ij}f(j)$ y aplicando  desigualdad triangular deducimos que
\[\big|f(i)-\hat p^{(n)}_{ii_0}f(i_0)\big|\leq\sum_{j\neq i_0}\hat p^{(n)}_{ij}|f(j)|\leq\|f\|_\infty(1-\hat p^{(n)}_{ii_0}).\]
Como consecuencia, es suficiente demostrar que $\lim_{n\to\infty}\hat p^{(n)}_{ii_0}=1$ pues, en ese caso, la desigualdad anterior entrega (en el límite $n\to\infty$) $|f(i)-f(i_0)|\leq0$, que implica lo requerido.

Demostremos entonces que $\hat p^{(n)}_{ii_0}$ converge a $1$ cuando $n\to\infty$.
El caso $i=i_0$ es trivial (y de hecho no se necesita en el argumento de arriba), así que tomemos $i\neq i_0$.
Sea $\hat\pp$ la medida de probabilidad asociada a la cadena $\hat X\sim\CM(\hat P)$ y $\hat\tau_{i_0}$ el tiempo de retorno a $i_0$ de $\hat X$.
\begin{exer}
Demuestre que $\pp_i(\tau_{i_0}<\infty)=\hat\pp_i(\hat\tau_{i_0}<\infty)$.
\end{exer}
\noindent Luego, por hipótesis de recurrencia e irreducibilidad, tenemos $\hat\pp_i(\hat\tau_{i_0}<\infty)=1$.
Pero como $i_0$ es absorbente para $\hat X$, es decir, una vez que $\hat X$ llega a $i_0$, se queda ahí para siempre, deducimos que
\NAM[
\[\hat\pp_i(\hat\tau_{i_0}<\infty)=\pp\big(\cup_{N\geq0}\{\exists\,n\leq N\!:\hat X_n=i_0\}\big)=\lim_{N\to\infty}\pp(\exists\,n\leq N\!:\hat X_n=i_0)=\lim_{N\to\infty}\pp(\hat X_N=i_0).\]
]{
\begin{multline*}
\hat\pp_i(\hat\tau_{i_0}<\infty)
=\pp\big(\cup_{N\geq0}\{\exists\,n\leq N\!:\hat X_n=i_0\}\big)\\
=\lim_{N\to\infty}\pp(\exists\,n\leq N\!:\hat X_n=i_0)=\lim_{N\to\infty}\pp(\hat X_N=i_0).
\end{multline*}
}

\uexer: Justifique las igualdades anteriores.
Como el lado izquierdo es igual a $1$, el límite también lo eso, y obtenemos lo deseado.
\end{proof}

\begin{ex}\label{ex:srw-rec-2}
Estudiemos nuevamente la marcha aleatoria simple en $\zz$.
Consideremos primero el caso simétrico, $p=\frac12$.
Elegimos $i_0=0$ y suponemos que $f$ es acotada y armónica en $\zz\setminus\{0\}$, es decir, $Pf(i)=f(i)$ para todo $i\neq0$, que se escribe como
\[\tfrac12f(i+1)+\tfrac12f(i-1)=f(i).\]
La recurrencia anterior puede entonces para $i\geq2$ (\uexers: hágalo), y obtenemos
\[f(i)=(f(1)-f(0))i+f(0)\quad\forall\,i\geq0.\]
Pero $f$ es acotada, por lo que necesariamente $f(1)=f(0)$ y entonces $f(i)=f(0)$ para todo $i\geq0$.
El mismo argumento puede aplicarse para $i\leq0$, y deducimos que $f\equiv f(0)$.
Luego, gracias al teorema, concluimos que $X$ es recurrente, como ya sabíamos del Ejemplo \ref{ex:srw-rec-1}.

\noindent Consideremos ahora el caso asimétrico.
Por simetría basta considerar $p>\frac12$.
Del Ejemplo \ref{ex:srw-rec-1} sabemos que la cadena es ahora transiente, por lo que debiera existir una función $f$ que es acotada y que es armónica en $\zz\setminus\{0\}$, pero que no es constante.
Veamos que esto es cierto.
Para ellos definimos
\[f(n)=\left(\frac{1-p}{p}\right)^{n}\uno{n>0}+\uno{n\leq0},\]
que es claramente es acotada y no constante.
Para $n>0$ tenemos
\begin{align}
Pf(n)&=(1-p)f(n-1)+pf(n+1)=(1-p)\left(\frac{1-p}{p}\right)^{n-1}+p\left(\frac{1-p}{p}\right)^{n+1}\\
&=\left(\frac{1-p}{p}\right)^{n}\big(p+(1-p)\big)=f(n).
\end{align}
Por otro lado, la igualdad $Pf(n)=f(n)$ para $n<0$ se satisface trivialmente.
Deducimos entonces que $f$ es armónica en $\zz\setminus\{0\}$, como queríamos.
\end{ex}

Un criterio alternativo al anterior, cuya demostración dejamos de ejercicio, es el siguiente:

\begin{thm}\tbf{Caracterización de recurrencia, versión alternativa}
Sea\\ $X\sim~\CM(P)$~irreducible.
Entonces $X$ es recurrente si y solo si las únicas funciones superarmónicas y no-negativas en $I$ son las constantes.
\end{thm}

\begin{exer}
Utilice el criterio anterior para escribir una demostración alternativa del resultado del Corolario \ref{cor:finito-rec} que afirma que toda cadena irreducible a valores en $I$ finito es recurrente.
Por simplicidad, si lo desea restrínjase al caso $|I|=3$.
\end{exer}

\subsection{Recurrencia positiva y distribuciones invariantes}

\begin{defn}\tbf{Medida invariante}
Sea $P$ una matriz estocástica y $\mu$ una medida sobre $I$ (que interpretamos como un vector fila indexado por $I$).
Decimos que $\mu$ es una \emph{medida invariante para $P$} (o \emph{medida $P$-invariante}) si
\[\mu P=\mu\]
(es decir, $\sum_i\mu_{i}p_{ij}=\mu_j$ para todo $j\in I$).
Si $\mu$ es además una medida de probabilidad, decimos que es una \emph{distribución invariante}.
Diremos también que $\mu$ es medida o distribución invariante para una cadena $X$ si lo es para su matriz de transición.
\end{defn}

Es claro que si $\mu$ es medida invariante, entonces cualquier múltiplo de ella lo es.

Como veremos en esta y la próxima sección, la noción de medida invariante juega un rol muy importante en la teoría de cadenas de Markov.
La relevancia de la definición (y la explicación de la terminología usada) es evidente, al menos para distribuciones invariantes: si $\mu$ es una distribución para $X\sim\CM(P)$, entonces $\mu P^n=\mu$ para todo $n\geq0$, y entonces
\[\pp_\mu(X_n=i)=\mu(i)\quad\forall\,i\in I,~\forall\,n\geq0.\]
Las distribuciones invariantes se conocen a veces también como \emph{distribuciones estacionarias} o \emph{distribuciones de equilibrio}.
El siguiente ejercicio explica en parte este último nombre:

\begin{exer}
Sea $I$ finito.
Supongamos que existe un $i\in I$ tal que los límites
\[\pi_j=\lim_{n\to\infty}p^{(n)}_{ij}\]
existen para todo $j$.
Entonces $\pi$ es un distribución invariante.
¿Qué puede decir del caso $I$ infinito? 
(Para esta última parte piense en una marcha aleatoria asimétrica y recuerde el Ejemplo \ref{ex:srw-rec-1}).
\end{exer}

\begin{rem}
Como $P$ es matriz estocástica, necesariamente tiene al $1$ como valor propio, asociado al vector propio por la derecha $v\equiv1$ (esto es $Pv=v$).
Como $1$ es valor propio, se sigue que debe tener asociado también un vector propio por la izquierda $\hat v$, $\hat v^{\sf T}P=\hat v^{\sf T}$ (al menos si $P$ fuera una matriz finita, pero lo mismo vale en el caso $I$ numerable).
Esto nos dice que $P$ siempre tiene una medida con signo invariante; buscar una medida invariante corresponde a buscar un vector propio no-negativo de $P$ por la izquierda, mientras que buscar una distribución invariante corresponde a pedir que este vector propio esté además en $\ell^1(I)$ (\uexers: ¿por qué?).
\end{rem}

En vista de la observación anterior, no tenemos por qué esperar que toda cadena de Markov tenga una medida invariante asociada, ni mucho menos una distribución invariante.
Este es uno de los puntos centrales que queremos dilucidar en esta sección.
Antes, veamos dos ejemplos donde estas medidas existen.

\begin{ex}
Para la marcha aleatoria simple con $p\in(0,1)$, la medida uniforme en $\zz$, $\mu\equiv1$, es invariante.
Sin embargo, esta medida no es finita, por lo que no puede normalizarse a una distribución invariante (más adelante estudiaremos la pregunta de si esta cadena posee o no una distribución invariante).
\end{ex}

\begin{ex}
Consideremos ahora la cadena de Ehrenfest del Ejemplo \ref{ex:ehrenfest}.
Aquí $I=\{0,\dotsc,k\}$ y $P$ está dada por $p_{k,k+1}=(r-k)/r$ y $p_{k,k-1}=k/r$, todo el resto de la matriz es cero.
En este caso hay una distribución invariante $\mu$, y es la que corresponde a una variable aleatoria Binomial$[r,\frac12]$, es decir
\[\mu(k)=\frac1{2^k}\binom{r}{k}.\]
Esto puede verificarse directamente, pero también puede entenderse de manera intuitiva: $\mu$ corresponde a elegir la urna en que se coloca cada bola de acuerdo a lanzamientos independientes de $r$ monedas, y cambiar una bola de urna corresponde a elegir una moneda al azar y darla vuelta.
\uexer: Demuestre directamente que $\mu$ es distribución invariante y asegúrese de comprender la explicación intuitiva.
\end{ex}

\begin{exer}
Considere una \emph{marcha aleatoria simple con reflexión} en $\zz_{\geq0}$, definida de igual forma que la marcha aleatoria usual (saltando a la derecha con probabilidad $p$ y a la izquierda con probabilidad $1-p$), salvo que cuando la cadena intenta pasar a $-1$, se le fuerza a quedarse en cero en vez.
Determine condiciones para la existencia de una distribución invariante y calcule dicha distribución (el Ejemplo \ref{ex:srw-rec-2} podría servirle de inspiración).
\end{exer}

\begin{exer}
Analice la existencia de medidas y distribuciones invariantes para la ruina del jugador (Ejemplo \ref{ex:ruina}).
\end{exer}

El principal resultado de esta sección mostrará que la existencia de distribuciones invariantes está íntimamente relacionada con una noción más fuerte de recurrencia, que definimos a continuación:

\begin{defn}\tbf{Recurrencia positiva}
Sea $i$ un estado recurrente.
Decimos que $i$ es \emph{recurrente positivo} si su tiempo de retorno esperado es finito, es decir, si $\ee_i(\tau_i)<\infty$.
En caso contrario, decimos que $i$ es \emph{recurrente nulo}.
\end{defn}

\begin{thm}\tbf{Existencia de distribuciones invariantes}\label{thm:ex-inv}
Sea $X$ irreducible.
Entonces las siguientes son equivalentes:
\begin{enumerate}[label=\uptext{(\roman*)}]
\item Existe un estado recurrente positivo.
\item Todos los estados son recurrentes positivos.
\item Existe una distribución invariante $\pi$.
\end{enumerate}
Más aún, en este caso se tiene que la distribución invariante está dada por
\[\pi(i)=\frac{1}{\ee_i(\tau_i)}.\]
\end{thm}

La demostración de este teorema nos tomará bastante trabajo.
Antes de pasar a ella enunciamos un resultado de unicidad en el contexto anterior.

\begin{thm}\label{thm:uniq-inv}
Sea $X$ irreducible.
Si $X$ tiene una distribución invariante $\pi$, entonces toda medida invariante es un múltiplo de $\pi$.
En particular, la distribución invariante es única.
\end{thm}

De estos dos teoremas se sigue directamente el siguiente:

\begin{cor}
Si $X$ es irreducible y tiene una medida invariante con masa infinita, entonces no existe distribución invariante (y en particular la cadena no es recurrente positiva).
\end{cor}

De esta forma tenemos, para $X$ irreducible:
\begin{enumerate}[label=\arabic*.]
\item La existencia de una distribución invariante equivale a que la cadena sea recurrente positiva.
\item Si existe una medida invariante infinita, entonces la cadena es recurrente nula o transiente.
\item Se puede demostrar también (\ucmark) que si $X$ es recurrente nula entonces existe una medida invariante infinita.
\end{enumerate}

\begin{proof}[Demostración Teorema \ref{thm:uniq-inv}]
Sea $\pi$ una distribución invariante.
Gracias al Teorema \ref{thm:ex-inv} tenemos $\pi(i)>0$ para todo $i\in I$.
Fijemos ahora una función $\varphi\!:[0,\infty)\longrightarrow\rr$ estrictamente convexa (la elección particular no es importante) y acotada.
Dada una medida $\mu$ sobre $I$, definimos
\[\mathcal{E}(\mu)=\sum_{j\in I}\varphi\Big(\frac{\mu(j)}{\pi(j)}\Big)\pi(j).\]
La demostración se basa en estudiar $\mathcal{E}(\mu P)$.
Partimos escribiendo
\[\mathcal{E}(\mu P)=\sum_{j}\varphi\Big(\frac{\sum_i\mu(i)p_{ij}}{\pi(j)}\Big)\pi(j)
=\sum_{j}\varphi\Big(\sum_i\frac{p_{ij}\pi(i)}{\pi(j)}\frac{\mu(i)}{\pi(i)}\Big)\pi(j).\]
Ahora $\frac{\sum_ip_{ij}\pi(i)}{\pi(j)}=\frac{(\pi P)_j}{\pi(j)}=1$, pues $\pi$ es invariante.
Luego, como $\varphi$ es convexa (y usando Teorema de Convergencia Dominada), tenemos
\[\mathcal{E}(\mu P)\leq\sum_{j}\sum_i\frac{p_{ij}\pi(i)}{\pi(j)}\varphi\Big(\frac{\mu(i)}{\pi(i)}\Big)\pi(j)=\sum_{j}\sum_ip_{ij}\pi(i)\varphi\Big(\frac{\mu(i)}{\pi(i)}\Big)
=\sum_i\pi(i)\varphi\Big(\frac{\mu(i)}{\pi(i)}\Big)=\mathcal{E}(\mu).\]
Supongamos ahora que $p_{ij}>0$ para todo $i,j$.
Entonces como $\varphi$ es estrictamente convexa, la desigualdad anterior es estricta a menos que $\frac{\mu(i)}{\pi(i)}$ sea constante.
Como $\mu$ es medida invariante, obviamente tenemos $\mathcal{E}(\mu P)=\mathcal{E}(\mu)$, y entonces deducimos que necesariamente $\frac{\mu(i)}{\pi(i)}$ es constante.
El paso al caso general se deja como el siguiente:
\begin{exer}
Concluya la demostración.
Para ello, considere la matriz de transición $\bar p_{ij}=\sum_{n\geq0}2^{-n}p^{(n)}_{ij}$.\qedhere
\end{exer}
\end{proof}

\begin{rem}
La cantidad $\mathcal{E}(\mu)$ que introdujimos en la demostración anterior se conoce usualmente como la \emph{entropía relativa} o la \emph{divergencia de Kullback-Leibler} de $\mu$ con respecto a $\pi$ (típicamente con la elección de $\varphi(x)=-\log(x)$, aunque en este caso levantamos el requerimiento de que $\varphi$ sea acotada, y extendemos la definición a $0$ mediante considerar $\lim_{x\to0^+}x\log(x)=0$).
Esta noción tiene muchas aplicaciones en probabilidades y otras áreas de las matemáticas, así como también a la teoría de la información, la estadística y el aprendizaje de máquinas.

\noindent La idea es que $\mathcal{E}(\mu)$ mide la distancia, o más precisamente el desorden, de $\mu$ con respecto a $\pi$ (note por ejemplo que $\mathcal{E}(\pi)=0$ para el caso $\varphi(x)=-\log(x)$).
Una consecuencia de la demostración anterior es la siguiente: la entropía relativa de cualquier distribución $\mu$ (no necesariamente invariante) con respecto a $\pi$ decrece luego de aplicar $P$.
Esto sugiere que las iteraciones sucesivas de una cadena de Markov acercan su distribución a la distribución de equilibrio $\pi$; esto será el tema de la siguiente sección.
\end{rem}

Comenzamos ahora con la demostración del Teorema \ref{thm:ex-inv}.
El primer paso es  la siguiente:

\begin{prop}\label{prop:timeconv}
Sea $P$ irreducible y recurrente.
Sea $N_n(i)=\sum_{k=0}^n\uno{X_k=i}$ el número de visiras a $i$ hasta tiempo $n$.
Entonces
\[\lim_{n\to\infty}\frac{N_n(i)}{n}=\frac1{\ee_i(\tau_i)}\quad\uptext{c.s.}\]
\end{prop}

\begin{proof}
Sea $\tau^{(k)}_i$ el tiempo del $k$-ésimo retorno $i$, introducido en la Definición \ref{def:k-retorno}.
Supondremos que $X_0=i$, el caso general queda de ejercicio.
Por propiedad de Markov fuerte, las variables aleatorias $(\tau^{(k+1)}_i-\tau^{(k)}_i)_{k\geq0}$ son i.i.d., y todas con distribución igual a la de $\tau_i$.
Luego
\begin{equation}
\frac{\tau^{(n)}_i}{n}=\frac1{n}\sum_{k=0}^{n-1}(\tau^{(k+1)}_i-\tau^{(k)}_i)
\xrightarrow[n\to\infty]{\uptext{c.s.}}\ee_i(\tau_i)\label{eq:lgn}
\end{equation}
por Ley de los Grandes Números.
Pero necesariamente tenemos $\tau^{(N_n(i))}_i\leq n<\tau^{(N_n(i)+1)}_i$, y luego
\[\frac{\tau^{(N_n(i))}_i}{N_n(i)}\leq\frac{n}{N_n(i)}<\frac{\tau^{(N_n(i)+1)}_i}{N_n(i)+1}\frac{N_n(i)+1}{N_n(i)}.\]
Como $X$ es recurrente, $N_n(i)\to\infty$, y luego de \eqref{eq:lgn} obtenemos 
\[\frac{\tau_i^{(N_n(i))}}{N_n(i)}\longrightarrow\ee_i(\tau_i)\quad\uptext{c.s.},\]
y lo mismo para el lado derecho.
Concluimos entonces $\frac{n}{N_n(i)}\longrightarrow\ee_i(\tau_i)$ casi seguramente, lo que nos da lo requerido.

\begin{exer}
Concluya la demostración, adaptando el argumento anterior al caso $X_0\neq i$.\qedhere
\end{exer}
\end{proof}

Para el próximo resultado introducimos, para cada $k\in I$,
\[\gamma_i^k=\ee_k\!\left(\sum_{n=0}^{\tau_k-1}\uno{X_n=i}\right)=\sum_{n\geq0}\pp_k(X_n=i,\,\tau_k>n),\]
el número de visitas a $i$ antes de retornar a $k$.

\begin{prop}\label{prop:gammaik}
Sea $P$ irreducible y recurrente.
\begin{enumerate}[label=\uptext{(\alph*)}]
\item $\gamma^k_k=1$.
\item $\gamma^k$ es medida invariante (es decir, $\gamma^kP=\gamma^k$).
\item $\gamma^k_i\in(0,\infty)$ para todo $i$.
\end{enumerate}
Más aún, $\mu=\gamma^k$ es la única medida invariante tal que $\mu_k=1$.
\end{prop}

\begin{proof}
La parte (a) es directa.
Para la parte (b) definimos $\bar p^{(n)}_{ki}= \pp_k(X_n=i,\,\tau_k>n)$ y notamos que
\[(\gamma^kP)_\ell=\sum_{j\in I}\gamma^k_jp_{j\ell}=\sum_{j\in I}\sum_{n\geq0}\bar p^{(n)}_{kj}p_{j\ell}.\]
Necesitamos considerar dos casos.
Primero, si $\ell\neq k$, lo anterior y la propiedad de Markov nos da
\begin{align}
(\gamma^kP)_\ell&=\sum_{j\in I}\sum_{n\geq0}\pp_k(X_n=j,\,\tau_k>n,\,X_{n+1}=\ell)=\sum_{n\geq0}\pp_k(\tau_k>n+1,\,X_{n+1}=\ell)\\
&=\sum_{n\geq0}\bar p^{(n+1)}_{k\ell}=\sum_{n\geq1}\bar p^{(n)}_{k\ell}
\end{align}
(\uexers: justifique la primera igualdad).
Pero $\bar p^{(0)}_{k\ell}=\pp_k(X_0=\ell,\,\tau_k>0)=0$ para $\ell\neq k$ y por lo tanto la última suma puede extenderse hasta $n=0$, de modo que la suma es igual a $\gamma^k_\ell$ como queríamos.
Supongamos ahora que $\ell=k$.
Argumentando como antes tenemos
\begin{equation}
(\gamma^kP)_k=\sum_{j\in I}\sum_{n\geq0}\pp_k(X_n=j,\,\tau_k>n,\,X_{n+1}=k)=\sum_{n\geq0}\pp_k(\tau_k>n,\,X_{n+1}=k).
\end{equation}
El término $n$-ésimo del lado derecho es igual a $\pp_k(\tau_k=n+1)$, y entonces la suma completa es igual a $\pp_k(1\leq\tau_k<\infty)=1=\gamma^k_k$ por (a) y gracias a que $P$ es recurrente.

Para (iii), como la cadena es irreducible podemos elegir un $n>0$ tal que $p_{ik}^{(n)}>0$, y entonces $1=\gamma^k_k=(\gamma^kP^n)_k\leq\gamma^k_ip_{ik}^{(n)}$ y deducimos que $\gamma^k_i>0$.
\begin{exer}
Use un argumento similar para probar que $\gamma^k_i<\infty$.
\end{exer}

Veamos ahora la unicidad.
Supongamos que $\mu$ es una medida invariante con $\mu_k=1$.
Iterando la relación $\mu P=\mu$ obtenemos, para $j\neq k$,
\begin{align}
\mu_j&=\sum_{i_0\in I}\mu_{i_0}p_{i_0j}=\sum_{i_0\neq k}\mu_{i_0}p_{i_0j}+p_{kj}=\sum_{i_0,i_1\neq k}\mu_{i_1}p_{i_1i_0}p_{i_0j}+p_{kj}+\sum_{i_0\neq k}p_{ki_0}p_{i_0j}\\
\shortintertext{e, inductivamente,}
&=\sum_{i_0,\dotsc,i_n\neq k}\mu_{i_n}p_{i_ni_{n-1}}\dotsm p_{i_0j}+p_{kj}+\sum_{i_0\neq k}p_{ki_0}p_{i_0j}+\dotsm+\sum_{i_0,\dotsc,i_{n-1}\neq k}p_{ki_{n-1}}\dotsm p_{i_0j}.
\shortintertext{Omitiendo el primer término, el lado derecho es (recordando $j\neq k$)}
&\geq\pp_k(X_1=j,\,\tau_k>1)+\pp_k(X_2=j,\,\tau_k>2)+\dotsm+\pp_k(X_n=j,\,\tau_k>n)\xrightarrow[n\to\infty]{}\gamma^k_j.
\end{align}
Deducimos entonces que $\mu_j\geq\gamma^k_j$ (notar que hasta acá no hemos usado recurrencia, y luego esta desigualdad es cierta en general).
Como $P$ es recurrente, $\gamma^k$ es medida invariante por (b), y luego la medida $\lambda=\mu-\gamma^k$ (notar que $\lambda\geq0$) también lo es.
Luego para $i\in I$ y cualquier $n>0$ tenemos $\lambda_ip_{ik}^{(n)}\leq(\lambda P)_k=\lambda_k=0$ (por hipótesis y (a)), y entonces usando la irreducibilidad para elegir $n>0$ tal que $p^{(n)}_{ik}>0$ obtenemos $\lambda_i=0$.
\end{proof}

\begin{proof}[Demostración Teorema \ref{thm:ex-inv}]
Veamos primero que (i) implica (iii).
Sea $k$ un estado recurrente positivo.
Gracias a la proposición, $\gamma^k$ es una medida invariante.
Basta probar entonces que $\gamma^k(I)<\infty$.
Tenemos
\begin{align}
\gamma^k(I)&=\sum_{\ell\in I}\gamma^k_\ell=\sum_{\ell\in I}\sum_{n\geq0}\pp_k(X_n=\ell,\,\tau_k>n)=\sum_{n\geq0}\pp_k(\tau_k>n)=\ee_k(\tau_k)<\infty,
\end{align}
gracias a la recurrencia positiva de $k$.

Que (ii) implica (i) es trivial.
Para ver que (iii) implica (ii), sea $\pi$ una distribución invariante y fijemos $k\in I$.
Gracias al Teorema \ref{thm:uniq-inv}, $\gamma^k=\alpha\pi$ para algún $\alpha\geq0$.
Tal como en la parte anterior, tenemos $\ee_k(\tau_k)=\gamma^k(I)$, y luego $\ee_k(\tau_k)=\alpha\tts\pi(I)=\alpha<\infty$, y luego $k$ es recurrente positivo.
\end{proof}

Antes de terminar esta sección, enunciamos un resultado que puede demostrarse usando los métodos de esta sección (\ucmark):

\begin{thm}\label{thm:ergodMC}\tbf{Teorema Ergódico para Cadenas de Markov}
Sea $P$ irreducible con distribución invariante $\pi$.
Sea $f\!:I\longrightarrow\rr$ tal que $\sum_i|f(i)|\pi(i)<\infty$ (es decir $f\in L^1(I,\pi)$).
Entonces
\[\lim_{n\to\infty}\frac1{n+1}\sum_{k=0}^nf(X_k)=\sum_if(i)\pi(i).\]
\end{thm}

Este tipo de resultados se conocen también como leyes de grandes números para funcionales aditivos (en este caso, de una cadena de Markov recurrente positiva).
Bajo hipótesis adicionales sobre $f$ y $X$, es posible también demostrar una versión del Teorema Central del Límite en este contexto.